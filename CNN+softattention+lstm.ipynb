{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmiYziXIQBOD",
        "outputId": "182c0bf2-1a52-4082-b098-e86d62a243ca"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrA-CdP8QIni",
        "outputId": "d2c46620-304e-4023-ab49-5c5e70632610"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrkMkSAlQt4j",
        "outputId": "d99378c7-e43a-4ba9-cdb8-1b7f032d4e52"
      },
      "outputs": [],
      "source": [
        "pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLLQNSxoQy25",
        "outputId": "38f4dbdc-1221-4d71-e3a0-0dd5d677497f"
      },
      "outputs": [],
      "source": [
        "import splitfolders\n",
        "import os\n",
        "\n",
        "input_folder=\"/content/drive/MyDrive/Defense/Datase/dataset2\"\n",
        "\n",
        "\n",
        "output=\"/content/\"\n",
        "\n",
        "\n",
        "\n",
        "splitfolders.ratio(input_folder, output, seed=42, ratio=(.8,.1,.1)) ### train 80%, val 10%, test 20%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C2SxNJvQ-OC",
        "outputId": "8d8f628a-cb81-43b8-978f-389b8af82af6"
      },
      "outputs": [],
      "source": [
        "pip install keras_applications==1.0.4 --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loBDE8dISOZW"
      },
      "outputs": [],
      "source": [
        "#Soft Attention\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer,InputSpec\n",
        "import keras.layers as kl\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "class SoftAttention(Layer):\n",
        "    def __init__(self,ch,m,concat_with_x=False,aggregate=False,**kwargs):\n",
        "        self.channels=int(ch)\n",
        "        self.multiheads = m\n",
        "        self.aggregate_channels = aggregate\n",
        "        self.concat_input_with_scaled = concat_with_x\n",
        "\n",
        "\n",
        "        super(SoftAttention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "\n",
        "        self.i_shape = input_shape\n",
        "\n",
        "        kernel_shape_conv3d = (self.channels, 3, 3) + (1, self.multiheads) # DHWC\n",
        "\n",
        "        self.out_attention_maps_shape = input_shape[0:1]+(self.multiheads,)+input_shape[1:-1]\n",
        "\n",
        "        if self.aggregate_channels==False:\n",
        "\n",
        "            self.out_features_shape = input_shape[:-1]+(input_shape[-1]+(input_shape[-1]*self.multiheads),)\n",
        "        else:\n",
        "            if self.concat_input_with_scaled:\n",
        "                self.out_features_shape = input_shape[:-1]+(input_shape[-1]*2,)\n",
        "            else:\n",
        "                self.out_features_shape = input_shape\n",
        "\n",
        "\n",
        "        self.kernel_conv3d = self.add_weight(shape=kernel_shape_conv3d,\n",
        "                                        initializer='he_uniform',\n",
        "                                        name='kernel_conv3d')\n",
        "        self.bias_conv3d = self.add_weight(shape=(self.multiheads,),\n",
        "                                      initializer='zeros',\n",
        "                                      name='bias_conv3d')\n",
        "\n",
        "        super(SoftAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        exp_x = K.expand_dims(x,axis=-1)\n",
        "\n",
        "        c3d = K.conv3d(exp_x,\n",
        "                     kernel=self.kernel_conv3d,\n",
        "                     strides=(1,1,self.i_shape[-1]), padding='same', data_format='channels_last')\n",
        "        conv3d = K.bias_add(c3d,\n",
        "                        self.bias_conv3d)\n",
        "        conv3d = kl.Activation('relu')(conv3d)\n",
        "\n",
        "        conv3d = K.permute_dimensions(conv3d,pattern=(0,4,1,2,3))\n",
        "\n",
        "\n",
        "        conv3d = K.squeeze(conv3d, axis=-1)\n",
        "        conv3d = K.reshape(conv3d,shape=(-1, self.multiheads ,self.i_shape[1]*self.i_shape[2]))\n",
        "\n",
        "        softmax_alpha = K.softmax(conv3d, axis=-1)\n",
        "        softmax_alpha = kl.Reshape(target_shape=(self.multiheads, self.i_shape[1],self.i_shape[2]))(softmax_alpha)\n",
        "\n",
        "\n",
        "        if self.aggregate_channels==False:\n",
        "            exp_softmax_alpha = K.expand_dims(softmax_alpha, axis=-1)\n",
        "            exp_softmax_alpha = K.permute_dimensions(exp_softmax_alpha,pattern=(0,2,3,1,4))\n",
        "\n",
        "            x_exp = K.expand_dims(x,axis=-2)\n",
        "\n",
        "            u = kl.Multiply()([exp_softmax_alpha, x_exp])\n",
        "\n",
        "            u = kl.Reshape(target_shape=(self.i_shape[1],self.i_shape[2],u.shape[-1]*u.shape[-2]))(u)\n",
        "\n",
        "        else:\n",
        "            exp_softmax_alpha = K.permute_dimensions(softmax_alpha,pattern=(0,2,3,1))\n",
        "\n",
        "            exp_softmax_alpha = K.sum(exp_softmax_alpha,axis=-1)\n",
        "\n",
        "            exp_softmax_alpha = K.expand_dims(exp_softmax_alpha, axis=-1)\n",
        "\n",
        "            u = kl.Multiply()([exp_softmax_alpha, x])\n",
        "\n",
        "        if self.concat_input_with_scaled:\n",
        "            o = kl.Concatenate(axis=-1)([u,x])\n",
        "        else:\n",
        "            o = u\n",
        "\n",
        "        return [o, softmax_alpha]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [self.out_features_shape, self.out_attention_maps_shape]\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(SoftAttention,self).get_config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU9YRJDTeGSs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,Dense,BatchNormalization,Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "model = Conv2D(32, kernel_size=(3, 3), strides=2, padding=\"same\", activation=\"relu\")(inputs)\n",
        "model = MaxPooling2D(pool_size=(2, 2))(model)\n",
        "model = Conv2D(64, kernel_size=(3, 3), strides=2, padding=\"same\", activation=\"relu\")(model)\n",
        "model = MaxPooling2D(pool_size=(2, 2))(model)\n",
        "model = Conv2D(64, kernel_size=(3, 3), strides=2, padding=\"same\", activation=\"relu\")(model)\n",
        "model = MaxPooling2D(pool_size=(2, 2))(model)\n",
        "model = MaxPooling2D(pool_size=(2, 2))(model)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gko52p8WSXSk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer,InputSpec\n",
        "import keras.layers as kl\n",
        "from glob import glob\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from  matplotlib import pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import concatenate,Dense, Conv2D, MaxPooling2D, Flatten,LSTM,Input,Activation,add,AveragePooling2D,BatchNormalization,Dropout,Reshape\n",
        "%matplotlib inline\n",
        "import shutil\n",
        "from sklearn.metrics import  precision_score, recall_score, accuracy_score,classification_report ,confusion_matrix\n",
        "from tensorflow.python.platform import build_info as tf_build_info\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "attention_layer,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(model.shape[-1]),name='soft_attention')(model)\n",
        "attention_layer=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(attention_layer))\n",
        "conv=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(model))\n",
        "\n",
        "conv = concatenate([conv,attention_layer])\n",
        "conv  = Activation('relu')(conv)\n",
        "conv = Dropout(0.5)(conv)\n",
        "\n",
        "\n",
        "output = Flatten()(conv)\n",
        "output = Reshape(target_shape=(16, 8))(output)\n",
        "output = LSTM(128, activation=\"tanh\")(output)\n",
        "output = Dense(4, activation='softmax')(output)\n",
        "model = Model(inputs=inputs, outputs=output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L2oDlKP4sf1",
        "outputId": "992be2d7-eabd-4a02-a595-299ac3277fbf"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2cSZfGUScaU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "\n",
        "IMAGE_SIZE = [224,224]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U78eYAfwSfBM"
      },
      "outputs": [],
      "source": [
        "op = keras.optimizers.Nadam(learning_rate=0.0007,epsilon=0.1)  \n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer=op,\n",
        "  metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rrNKUXGSjOY",
        "outputId": "cc004eec-ca23-47be-ddce-32dd4936d14b"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   brightness_range=(0.4, 0.7),\n",
        "                                   vertical_flip= True,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/train',\n",
        "                                                 target_size = (224,224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "val_set = val_datagen.flow_from_directory('/content/val',\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/test',\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 1,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "\n",
        "\n",
        "model.optimizer.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj7IqW1KSj75",
        "outputId": "e567143e-560a-4589-bcc9-45bc0d43e216"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "filepath = r\"/content/drive/MyDrive/Defense/Accuracy/CNN+softattention/mobilenetv2_SE_.h5\"\n",
        "\n",
        "\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_weights_only=True,\n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "log_csv = CSVLogger(r'/content/drive/MyDrive/Defense/Accuracy/CNN+softattention/mobilenetv2_SE_.csv', separator=',', append=False)\n",
        "\n",
        "callbacks_list = [checkpoint1,log_csv]\n",
        "\n",
        "\n",
        "r = model.fit_generator(\n",
        "    training_set,\n",
        "    epochs=160,\n",
        "    validation_data=val_set,\n",
        "    steps_per_epoch = len(training_set),\n",
        "    validation_steps=len(val_set),\n",
        "    callbacks=callbacks_list,\n",
        "    shuffle=False\n",
        "\n",
        "\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3e-5Bx68SoJ",
        "outputId": "af15ea01-50b4-4cc3-9cf4-54b8ae7277e7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "filepath = r\"/content/drive/MyDrive/Defense/Accuracy/CNN+softattention/mobilenetv2_SE_.h5\"\n",
        "\n",
        "\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_weights_only=True,\n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "log_csv = CSVLogger(r'/content/drive/MyDrive/Defense/Accuracy/CNN+softattention/mobilenetv2_SE_.csv', separator=',', append=False)\n",
        "\n",
        "callbacks_list = [checkpoint1,log_csv]\n",
        "\n",
        "\n",
        "r = model.fit_generator(\n",
        "    training_set,\n",
        "    epochs=160,\n",
        "    validation_data=val_set,\n",
        "    steps_per_epoch = len(training_set),\n",
        "    validation_steps=len(val_set),\n",
        "    callbacks=callbacks_list,\n",
        "    shuffle=False\n",
        "\n",
        "\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgFsMNsy7mPZ",
        "outputId": "59150ce3-d81b-41c0-a0c3-704dbee758a0"
      },
      "outputs": [],
      "source": [
        "model.load_weights(r\"/content/drive/MyDrive/softattention/mobilenetv2_SE_.h5\")\n",
        "Adam = keras.optimizers.Adam(lr=0.001)\n",
        "\n",
        "\n",
        "preds = model.evaluate_generator(val_set)\n",
        "print (\"Loss = \",float(preds[0]))\n",
        "print (\"Test Accuracy = \",float(preds[1])*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDeRx4nnDrZK",
        "outputId": "e50e565b-9b6f-4028-c7d9-4b01892a349b"
      },
      "outputs": [],
      "source": [
        "model.load_weights(r\"/content/drive/MyDrive/softattention/mobilenetv2_SE_.h5\")\n",
        "Adam = keras.optimizers.Adam(lr=0.001)\n",
        "\n",
        "\n",
        "preds = model.evaluate_generator(test_set)\n",
        "print (\"Loss = \",float(preds[0]))\n",
        "print (\"Test Accuracy = \",float(preds[1])*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvMjuPgiD3z5"
      },
      "outputs": [],
      "source": [
        "### loss and accuracy curve ### updated !!!!!!!!!!\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fin1= pd.read_csv(r'/content/drive/MyDrive/softattention/mobilenetv2_SE_.csv') ###directory\n",
        "\n",
        "\n",
        "### accuracy curve ###\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(fin1['accuracy'], label='Trining accuracy')\n",
        "plt.plot(fin1['val_accuracy'], label='Validation accuracy')\n",
        "\n",
        "plt.title('Accuracy Curve')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.savefig(r'/content/drive/MyDrive/softattention/Zrms_accuracy.png', dpi = 300)\n",
        "\n",
        "### loss curve ###\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(fin1['loss'], label='Trining loss')\n",
        "plt.plot(fin1['val_loss'], label='Validation loss')\n",
        "\n",
        "plt.title('Loss Curve')\n",
        "\n",
        "plt.savefig(r'/content/drive/MyDrive/softattention/Zrms_loss.png', dpi = 300)\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ9xjnoVFZKG"
      },
      "outputs": [],
      "source": [
        "### for f1 score and other values import files ###\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#importing packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "### matrics\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PziSNaZrFkOc"
      },
      "outputs": [],
      "source": [
        "####predictions on test set\n",
        "\n",
        "y_test=[]\n",
        "probaa=[]\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "model.load_weights(r\"/content/drive/MyDrive/softattention/mobilenetv2_SE_.h5\")\n",
        "\n",
        "\n",
        "cls=4  #nummber of classes\n",
        "\n",
        "## give directory of class serially..if less than 4 classes comment out the last unused dir_path\n",
        "dir_path1=r'/content/drive/MyDrive/dataset2/Botrytis Blight'\n",
        "dir_path2=r'/content/drive/MyDrive/dataset2/Downy Mildew'\n",
        "dir_path3=r'/content/drive/MyDrive/dataset2/Purple Blotch'\n",
        "dir_path4=r'/content/drive/MyDrive/dataset2/Stemphylium leaf blight'\n",
        "\n",
        "\n",
        "w1=0\n",
        "w2=0\n",
        "w3=0\n",
        "w4=0\n",
        "\n",
        "\n",
        "if cls==5:\n",
        "  for filename in os.listdir(dir_path1):\n",
        "      img=image.load_img(dir_path1+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(0)\n",
        "      w1+=1\n",
        "      print(w1,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "  for filename in os.listdir(dir_path2):\n",
        "      img=image.load_img(dir_path2+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(1)\n",
        "      w2+=1\n",
        "      print(w2,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "  for filename in os.listdir(dir_path3):\n",
        "      img=image.load_img(dir_path3+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(2)\n",
        "      w3+=1\n",
        "      print(w3,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "  for filename in os.listdir(dir_path4):\n",
        "      img=image.load_img(dir_path4+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(3)\n",
        "      w4+=1\n",
        "      print(w4,end='_')\n",
        "\n",
        "\n",
        "  print('class numbers', cls)\n",
        "  print('prediction number:',len(probaa))\n",
        "  print('true labels:',len(y_test))\n",
        "\n",
        "if cls==4:\n",
        "  for filename in os.listdir(dir_path1):\n",
        "      img=image.load_img(dir_path1+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(0)\n",
        "      w1+=1\n",
        "      print(w1,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "  for filename in os.listdir(dir_path2):\n",
        "      img=image.load_img(dir_path2+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(1)\n",
        "      #if res!=1: a2+=1\n",
        "      w2+=1\n",
        "      print(w2,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "  for filename in os.listdir(dir_path3):\n",
        "      img=image.load_img(dir_path3+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(2)\n",
        "      w3+=1\n",
        "      print(w3,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "  for filename in os.listdir(dir_path4):\n",
        "      img=image.load_img(dir_path4+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(3)\n",
        "      w4+=1\n",
        "      print(w4,end='_')\n",
        "\n",
        "  print('class numbers', cls)\n",
        "  print('prediction number:',len(probaa))\n",
        "  print('true labels:',len(y_test))\n",
        "\n",
        "\n",
        "if cls==2:\n",
        "  for filename in os.listdir(dir_path1):\n",
        "      img=image.load_img(dir_path1+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(0)\n",
        "      w1+=1\n",
        "      print(w1,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "  for filename in os.listdir(dir_path2):\n",
        "      img=image.load_img(dir_path2+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(1)\n",
        "      #if res!=1: a2+=1\n",
        "      w2+=1\n",
        "      print(w2,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "\n",
        "  print('class numbers', cls)\n",
        "  print('prediction number:',len(probaa))\n",
        "  print('true labels:',len(y_test))\n",
        "\n",
        "\n",
        "if cls==3:\n",
        "  for filename in os.listdir(dir_path1):\n",
        "      img=image.load_img(dir_path1+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(0)\n",
        "      w1+=1\n",
        "      print(w1,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "  for filename in os.listdir(dir_path2):\n",
        "      img=image.load_img(dir_path2+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(1)\n",
        "      w2+=1\n",
        "      print(w2,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "  for filename in os.listdir(dir_path3):\n",
        "      img=image.load_img(dir_path3+'//'+filename, target_size=(224,224,3))\n",
        "      im = []\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      im.append(img)\n",
        "      X= np.array(im)\n",
        "      result=model.predict(X,verbose=0)\n",
        "      #training_set.class_indices\n",
        "      res=np.argmax(result)\n",
        "      probaa.append(res)\n",
        "      y_test.append(2)\n",
        "      w3+=1\n",
        "      print(w3,end='_')\n",
        "\n",
        "  print('\\n')\n",
        "\n",
        "  print('class numbers', cls)\n",
        "  print('prediction number:',len(probaa))\n",
        "  print('true labels:',len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wPwnaXIGOU5"
      },
      "outputs": [],
      "source": [
        "#We use Support Vector classifier as a classifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#importing packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "### matrics\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSLGmZ41GSxb"
      },
      "outputs": [],
      "source": [
        "##  make a directory for storing figures\n",
        "y_preD=probaa\n",
        "cm = confusion_matrix(y_test, y_preD)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index = ['Botrytis Blight','Downy Mildew','Purple Blotch','Stemphylium leaf blight'],\n",
        "                     columns = ['Botrytis Blight','Downy Mildew','Purple Blotch','Stemphylium leaf blight'])\n",
        "\n",
        "#Plotting the confusion matrix\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_df,cmap='cividis', annot=True, fmt='d')\n",
        "#plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actal Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "\n",
        "###save confusion matrix to fig directory ...keep the image file name as h5 file name. ex:  breastCancer_geometric.h5 >>> breastCancer_geometric_con.png\n",
        "plt.savefig(\"/content/drive/MyDrive/rhd_model/csv/confusion_matrix.png\", dpi = 300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRSjOqdaGsSU",
        "outputId": "c7144972-18e8-46ec-bcae-545837958adb"
      },
      "outputs": [],
      "source": [
        "y_preD=probaa\n",
        "y_val=y_test\n",
        "\n",
        "confusion_matrix=cm\n",
        "\n",
        "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
        "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
        "TP = np.diag(confusion_matrix)\n",
        "TN = len(y_val) - (FP + FN + TP)\n",
        "\n",
        "# Sensitivity, hit rate, recall, or true positive rate\n",
        "TPR = TP/(TP+FN)\n",
        "# Specificity or true negative rate\n",
        "TNR = TN/(TN+FP)\n",
        "# Precision or positive predictive value\n",
        "PPV = TP/(TP+FP)\n",
        "# Negative predictive value\n",
        "NPV = TN/(TN+FN)\n",
        "# Fall out or false positive rate\n",
        "FPR = FP/(FP+TN)\n",
        "# False negative rate\n",
        "FNR = FN/(TP+FN)\n",
        "# False discovery rate\n",
        "FDR = FP/(TP+FP)\n",
        "\n",
        "# Overall accuracy\n",
        "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "from numpy import mean\n",
        "\n",
        "MAE=mean_absolute_error(y_val, y_preD)\n",
        "MSE=mean_squared_error(y_val, y_preD)\n",
        "F1=2*((mean(PPV)*mean(TPR))/(mean(PPV)+mean(TPR)))\n",
        "\n",
        "#print(\"ACC=   \",mean(ACC)*100)\n",
        "\n",
        "print(\"Rec=   \",mean(TPR)*100)\n",
        "print(\"Spe=   \",mean(TNR)*100)\n",
        "print(\"Pre=   \",mean(PPV)*100)\n",
        "print(\"FPR=   \",mean(FPR)*100)\n",
        "print(\"FNR=   \",mean(FNR)*100)\n",
        "print(\"NPV=   \",mean(NPV)*100)\n",
        "print(\"FDR=   \",mean(FDR)*100)\n",
        "print(\"F1=    \",mean(F1)*100)\n",
        "\n",
        "print(\"MAE   =\",MAE*100)\n",
        "print(\"RMSE  =\",math.sqrt(MSE)*100)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAtqcGH5NrLi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
