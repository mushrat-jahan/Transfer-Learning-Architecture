{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_6NoN4QO1VM",
        "outputId": "547d32bf-1c57-4b98-e32a-7d5e51bb078f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jul  2 11:25:27 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKmkyzPxPRXZ",
        "outputId": "11b419bd-ed34-40b2-842d-3c26d51d1f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euMnDMCEO6ni"
      },
      "outputs": [],
      "source": [
        "# Import the inception model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkevXyB3O6qi",
        "outputId": "41518dc2-82fb-4489-ecd7-7dc89c988501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 111, 111, 32  864         ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 111, 111, 32  96         ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 111, 111, 32  0           ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 109, 109, 32  9216        ['activation[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 109, 109, 32  96         ['conv2d_1[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 109, 109, 32  0           ['batch_normalization_1[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 109, 109, 64  18432       ['activation_1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 109, 109, 64  192        ['conv2d_2[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 109, 109, 64  0           ['batch_normalization_2[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 54, 54, 64)   0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 54, 54, 80)   5120        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 54, 54, 80)  240         ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 54, 54, 80)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 52, 52, 192)  138240      ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 52, 52, 192)  576        ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 52, 52, 192)  0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0          ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 25, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 25, 25, 64)  192         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 25, 25, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 25, 25, 48)   9216        ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 25, 25, 96)   55296       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 25, 25, 48)  144         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 25, 25, 96)  288         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 25, 25, 48)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 25, 25, 96)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 25, 25, 192)  0          ['max_pooling2d_1[0][0]']        \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 25, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 25, 25, 64)   76800       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 25, 25, 96)   82944       ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 25, 25, 32)   6144        ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 25, 25, 64)  192         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 25, 25, 64)  192         ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 25, 25, 96)  288         ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 25, 25, 32)  96          ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 25, 25, 64)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 25, 25, 64)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 25, 25, 32)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)           (None, 25, 25, 256)  0           ['activation_5[0][0]',           \n",
            "                                                                  'activation_7[0][0]',           \n",
            "                                                                  'activation_10[0][0]',          \n",
            "                                                                  'activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 25, 25, 64)  192         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 25, 25, 48)   12288       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 25, 25, 96)   55296       ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 25, 25, 48)  144         ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 25, 25, 96)  288         ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 25, 25, 48)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 25, 25, 256)  0          ['mixed0[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 25, 25, 64)   76800       ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 25, 25, 96)   82944       ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 25, 25, 64)   16384       ['average_pooling2d_1[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 25, 25, 64)  192         ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 25, 25, 64)  192         ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 25, 25, 96)  288         ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 25, 25, 64)  192         ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)           (None, 25, 25, 288)  0           ['activation_12[0][0]',          \n",
            "                                                                  'activation_14[0][0]',          \n",
            "                                                                  'activation_17[0][0]',          \n",
            "                                                                  'activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 25, 25, 64)  192         ['conv2d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 25, 25, 48)   13824       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 25, 25, 96)   55296       ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 25, 25, 48)  144         ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 25, 25, 96)  288         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 25, 25, 48)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 25, 25, 288)  0          ['mixed1[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 25, 25, 64)   76800       ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 25, 25, 96)   82944       ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 25, 25, 64)   18432       ['average_pooling2d_2[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 25, 25, 64)  192         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 25, 25, 64)  192         ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 25, 25, 96)  288         ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 25, 25, 64)  192         ['conv2d_25[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)           (None, 25, 25, 288)  0           ['activation_19[0][0]',          \n",
            "                                                                  'activation_21[0][0]',          \n",
            "                                                                  'activation_24[0][0]',          \n",
            "                                                                  'activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 25, 25, 64)   18432       ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 25, 25, 64)  192         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 25, 25, 96)   55296       ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 25, 25, 96)  288         ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 12, 12, 384)  995328      ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 12, 12, 96)   82944       ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 12, 12, 384)  1152       ['conv2d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 12, 12, 96)  288         ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 12, 12, 384)  0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 12, 12, 96)   0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0          ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)           (None, 12, 12, 768)  0           ['activation_26[0][0]',          \n",
            "                                                                  'activation_29[0][0]',          \n",
            "                                                                  'max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 12, 12, 128)  384        ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 12, 12, 128)  384        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 12, 12, 128)  384        ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 12, 12, 128)  384        ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 12, 12, 128)  384        ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 12, 12, 128)  384        ['conv2d_37[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 12, 12, 768)  0          ['mixed3[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 12, 12, 192)  172032      ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 12, 12, 192)  172032      ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 12, 12, 192)  576        ['conv2d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 12, 12, 192)  576        ['conv2d_33[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 12, 12, 192)  576        ['conv2d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 12, 12, 192)  576        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)           (None, 12, 12, 768)  0           ['activation_30[0][0]',          \n",
            "                                                                  'activation_33[0][0]',          \n",
            "                                                                  'activation_38[0][0]',          \n",
            "                                                                  'activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 12, 12, 160)  480        ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 12, 12, 160)  480        ['conv2d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 12, 12, 160)  480        ['conv2d_41[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 12, 12, 160)  480        ['conv2d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 12, 12, 160)  480        ['conv2d_42[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 12, 12, 160)  480        ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_4 (AveragePo  (None, 12, 12, 768)  0          ['mixed4[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_4[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 12, 12, 192)  576        ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 12, 12, 192)  576        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 12, 12, 192)  576        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 12, 12, 192)  576        ['conv2d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)           (None, 12, 12, 768)  0           ['activation_40[0][0]',          \n",
            "                                                                  'activation_43[0][0]',          \n",
            "                                                                  'activation_48[0][0]',          \n",
            "                                                                  'activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 12, 12, 160)  480        ['conv2d_54[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_54[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 12, 12, 160)  480        ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_55[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 12, 12, 160)  480        ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 12, 12, 160)  480        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_51[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_56[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 12, 12, 160)  480        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 12, 12, 160)  480        ['conv2d_57[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_5 (AveragePo  (None, 12, 12, 768)  0          ['mixed5[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_57[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_5[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 12, 12, 192)  576        ['conv2d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 12, 12, 192)  576        ['conv2d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 12, 12, 192)  576        ['conv2d_58[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 12, 12, 192)  576        ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)           (None, 12, 12, 768)  0           ['activation_50[0][0]',          \n",
            "                                                                  'activation_53[0][0]',          \n",
            "                                                                  'activation_58[0][0]',          \n",
            "                                                                  'activation_59[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 12, 12, 192)  576        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 12, 12, 192)  576        ['conv2d_65[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_65[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, 12, 12, 192)  576        ['conv2d_61[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 12, 12, 192)  576        ['conv2d_66[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_61[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_66[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, 12, 12, 192)  576        ['conv2d_62[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 12, 12, 192)  576        ['conv2d_67[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " activation_67 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_6 (AveragePo  (None, 12, 12, 768)  0          ['mixed6[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_62[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_67[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_6[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, 12, 12, 192)  576        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 12, 12, 192)  576        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 12, 12, 192)  576        ['conv2d_68[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 12, 12, 192)  576        ['conv2d_69[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " activation_68 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " activation_69 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)           (None, 12, 12, 768)  0           ['activation_60[0][0]',          \n",
            "                                                                  'activation_63[0][0]',          \n",
            "                                                                  'activation_68[0][0]',          \n",
            "                                                                  'activation_69[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, 12, 12, 192)  576        ['conv2d_72[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_72 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_72[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_73 (BatchN  (None, 12, 12, 192)  576        ['conv2d_73[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_73 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_73[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 12, 12, 192)  576        ['conv2d_70[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_74 (BatchN  (None, 12, 12, 192)  576        ['conv2d_74[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_70 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " activation_74 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_74[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, 5, 5, 320)    552960      ['activation_70[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, 5, 5, 192)    331776      ['activation_74[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 5, 5, 320)   960         ['conv2d_71[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, 5, 5, 192)   576         ['conv2d_75[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, 5, 5, 320)    0           ['batch_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " activation_75 (Activation)     (None, 5, 5, 192)    0           ['batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)   0           ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)           (None, 5, 5, 1280)   0           ['activation_71[0][0]',          \n",
            "                                                                  'activation_75[0][0]',          \n",
            "                                                                  'max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)             (None, 5, 5, 448)    573440      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_80 (BatchN  (None, 5, 5, 448)   1344        ['conv2d_80[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_80 (Activation)     (None, 5, 5, 448)    0           ['batch_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)             (None, 5, 5, 384)    491520      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)             (None, 5, 5, 384)    1548288     ['activation_80[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_77[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_81 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_81[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_77 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " activation_81 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_77[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_77[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_81[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_81[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_7 (AveragePo  (None, 5, 5, 1280)  0           ['mixed8[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)             (None, 5, 5, 320)    409600      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_78[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_79[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_82 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_82[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_83 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_83[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)             (None, 5, 5, 192)    245760      ['average_pooling2d_7[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, 5, 5, 320)   960         ['conv2d_76[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_78 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " activation_79 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " activation_82 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " activation_83 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_83[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_84 (BatchN  (None, 5, 5, 192)   576         ['conv2d_84[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_76 (Activation)     (None, 5, 5, 320)    0           ['batch_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)         (None, 5, 5, 768)    0           ['activation_78[0][0]',          \n",
            "                                                                  'activation_79[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5, 5, 768)    0           ['activation_82[0][0]',          \n",
            "                                                                  'activation_83[0][0]']          \n",
            "                                                                                                  \n",
            " activation_84 (Activation)     (None, 5, 5, 192)    0           ['batch_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)           (None, 5, 5, 2048)   0           ['activation_76[0][0]',          \n",
            "                                                                  'mixed9_0[0][0]',               \n",
            "                                                                  'concatenate[0][0]',            \n",
            "                                                                  'activation_84[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)             (None, 5, 5, 448)    917504      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_89 (BatchN  (None, 5, 5, 448)   1344        ['conv2d_89[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_89 (Activation)     (None, 5, 5, 448)    0           ['batch_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)             (None, 5, 5, 384)    786432      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)             (None, 5, 5, 384)    1548288     ['activation_89[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_86 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_86[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_90 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_90[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_86 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " activation_90 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_86[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_86[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_90[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_90[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_8 (AveragePo  (None, 5, 5, 2048)  0           ['mixed9[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)             (None, 5, 5, 320)    655360      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_87 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_87[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_88 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_88[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_91 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_91[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_92 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_92[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)             (None, 5, 5, 192)    393216      ['average_pooling2d_8[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_85 (BatchN  (None, 5, 5, 320)   960         ['conv2d_85[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_87 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " activation_88 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " activation_91 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " activation_92 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_93 (BatchN  (None, 5, 5, 192)   576         ['conv2d_93[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_85 (Activation)     (None, 5, 5, 320)    0           ['batch_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " mixed9_1 (Concatenate)         (None, 5, 5, 768)    0           ['activation_87[0][0]',          \n",
            "                                                                  'activation_88[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 5, 5, 768)    0           ['activation_91[0][0]',          \n",
            "                                                                  'activation_92[0][0]']          \n",
            "                                                                                                  \n",
            " activation_93 (Activation)     (None, 5, 5, 192)    0           ['batch_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " mixed10 (Concatenate)          (None, 5, 5, 2048)   0           ['activation_85[0][0]',          \n",
            "                                                                  'mixed9_1[0][0]',               \n",
            "                                                                  'concatenate_1[0][0]',          \n",
            "                                                                  'activation_93[0][0]']          \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 51200)        0           ['mixed10[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 4)            204804      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 22,007,588\n",
            "Trainable params: 204,804\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Loading deep learning algorithm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "#import keras\n",
        "#import keras.backend as K\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "IMAGE_SIZE = [224,224]\n",
        "CLASS=4\n",
        "inception = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "#model = InceptionV3(weights='imagenet', include_top=False)\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False\n",
        "#folders = glob('C:\\rafid\\guava disease research\\k_guava\\train_image/*')\n",
        "x = Flatten()(inception.output)\n",
        "prediction = Dense(CLASS, activation='softmax')(x)\n",
        "model = Model(inputs=inception.input, outputs=prediction)\n",
        "adam = keras.optimizers.Adam(lr = 0.001)\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer = adam,\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "print(\"\\n\\n\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vov6M-XSO6tM",
        "outputId": "bfbbd4b3-3f24-4023-d01e-d6f02c7ad2f7"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/Research/defense/Datase/Split-dataset/train',\n",
        "                                                 target_size = (224,224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "val_set = val_datagen.flow_from_directory('/content/drive/MyDrive/Research/defense/Datase/Split-dataset/val',\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/Research/defense/Datase/Split-dataset/test',\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 1,\n",
        "                                            class_mode = 'categorical')\n",
        "print(\"\\n\\n\")\n",
        "model.optimizer.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBh4ALwFO6v7",
        "outputId": "9f2a1b50-4c9b-45d1-e066-a3f52214f73c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-5c29e9b04f25>:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  r = model.fit_generator(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 3.7127 - accuracy: 0.5957\n",
            "Epoch 1: val_accuracy improved from -inf to 0.72707, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5\n",
            "\n",
            "Epoch 1: accuracy improved from -inf to 0.59574, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 605s 8s/step - loss: 3.7127 - accuracy: 0.5957 - val_loss: 1.7472 - val_accuracy: 0.7271\n",
            "Epoch 2/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9232 - accuracy: 0.8045\n",
            "Epoch 2: val_accuracy improved from 0.72707 to 0.78482, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5\n",
            "\n",
            "Epoch 2: accuracy improved from 0.59574 to 0.80450, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 65s 836ms/step - loss: 0.9232 - accuracy: 0.8045 - val_loss: 1.1386 - val_accuracy: 0.7848\n",
            "Epoch 3/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0244 - accuracy: 0.8097\n",
            "Epoch 3: val_accuracy did not improve from 0.78482\n",
            "\n",
            "Epoch 3: accuracy improved from 0.80450 to 0.80971, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 65s 830ms/step - loss: 1.0244 - accuracy: 0.8097 - val_loss: 2.0419 - val_accuracy: 0.7259\n",
            "Epoch 4/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8014 - accuracy: 0.8551\n",
            "Epoch 4: val_accuracy did not improve from 0.78482\n",
            "\n",
            "Epoch 4: accuracy improved from 0.80971 to 0.85508, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 811ms/step - loss: 0.8014 - accuracy: 0.8551 - val_loss: 1.8470 - val_accuracy: 0.7486\n",
            "Epoch 5/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6189 - accuracy: 0.8747\n",
            "Epoch 5: val_accuracy did not improve from 0.78482\n",
            "\n",
            "Epoch 5: accuracy improved from 0.85508 to 0.87475, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 810ms/step - loss: 0.6189 - accuracy: 0.8747 - val_loss: 1.3549 - val_accuracy: 0.7826\n",
            "Epoch 6/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.8760\n",
            "Epoch 6: val_accuracy improved from 0.78482 to 0.82899, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5\n",
            "\n",
            "Epoch 6: accuracy improved from 0.87475 to 0.87595, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 65s 832ms/step - loss: 0.6362 - accuracy: 0.8760 - val_loss: 1.2116 - val_accuracy: 0.8290\n",
            "Epoch 7/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.8920\n",
            "Epoch 7: val_accuracy did not improve from 0.82899\n",
            "\n",
            "Epoch 7: accuracy improved from 0.87595 to 0.89201, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 813ms/step - loss: 0.6142 - accuracy: 0.8920 - val_loss: 1.7505 - val_accuracy: 0.8086\n",
            "Epoch 8/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6828 - accuracy: 0.8940\n",
            "Epoch 8: val_accuracy did not improve from 0.82899\n",
            "\n",
            "Epoch 8: accuracy improved from 0.89201 to 0.89402, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 813ms/step - loss: 0.6828 - accuracy: 0.8940 - val_loss: 1.8502 - val_accuracy: 0.7894\n",
            "Epoch 9/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.9016\n",
            "Epoch 9: val_accuracy improved from 0.82899 to 0.84032, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5\n",
            "\n",
            "Epoch 9: accuracy improved from 0.89402 to 0.90165, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 64s 826ms/step - loss: 0.5909 - accuracy: 0.9016 - val_loss: 1.5174 - val_accuracy: 0.8403\n",
            "Epoch 10/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7254 - accuracy: 0.8952\n",
            "Epoch 10: val_accuracy did not improve from 0.84032\n",
            "\n",
            "Epoch 10: accuracy did not improve from 0.90165\n",
            "78/78 [==============================] - 62s 795ms/step - loss: 0.7254 - accuracy: 0.8952 - val_loss: 1.6055 - val_accuracy: 0.8256\n",
            "Epoch 11/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.9097\n",
            "Epoch 11: val_accuracy did not improve from 0.84032\n",
            "\n",
            "Epoch 11: accuracy improved from 0.90165 to 0.90967, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 64s 823ms/step - loss: 0.5279 - accuracy: 0.9097 - val_loss: 3.0418 - val_accuracy: 0.7055\n",
            "Epoch 12/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9506 - accuracy: 0.8816\n",
            "Epoch 12: val_accuracy did not improve from 0.84032\n",
            "\n",
            "Epoch 12: accuracy did not improve from 0.90967\n",
            "78/78 [==============================] - 61s 780ms/step - loss: 0.9506 - accuracy: 0.8816 - val_loss: 1.8630 - val_accuracy: 0.8290\n",
            "Epoch 13/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4561 - accuracy: 0.9269\n",
            "Epoch 13: val_accuracy improved from 0.84032 to 0.84371, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5\n",
            "\n",
            "Epoch 13: accuracy improved from 0.90967 to 0.92694, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 65s 835ms/step - loss: 0.4561 - accuracy: 0.9269 - val_loss: 1.6980 - val_accuracy: 0.8437\n",
            "Epoch 14/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.9342\n",
            "Epoch 14: val_accuracy improved from 0.84371 to 0.88335, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5\n",
            "\n",
            "Epoch 14: accuracy improved from 0.92694 to 0.93416, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 64s 818ms/step - loss: 0.4111 - accuracy: 0.9342 - val_loss: 1.0143 - val_accuracy: 0.8834\n",
            "Epoch 15/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3873 - accuracy: 0.9346\n",
            "Epoch 15: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 15: accuracy improved from 0.93416 to 0.93456, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 807ms/step - loss: 0.3873 - accuracy: 0.9346 - val_loss: 1.5734 - val_accuracy: 0.8437\n",
            "Epoch 16/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.9249\n",
            "Epoch 16: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 16: accuracy did not improve from 0.93456\n",
            "78/78 [==============================] - 63s 803ms/step - loss: 0.4753 - accuracy: 0.9249 - val_loss: 2.7982 - val_accuracy: 0.8007\n",
            "Epoch 17/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.9458\n",
            "Epoch 17: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 17: accuracy improved from 0.93456 to 0.94580, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 806ms/step - loss: 0.3722 - accuracy: 0.9458 - val_loss: 2.2040 - val_accuracy: 0.8414\n",
            "Epoch 18/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5953 - accuracy: 0.9237\n",
            "Epoch 18: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 18: accuracy did not improve from 0.94580\n",
            "78/78 [==============================] - 62s 798ms/step - loss: 0.5953 - accuracy: 0.9237 - val_loss: 1.4378 - val_accuracy: 0.8788\n",
            "Epoch 19/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5715 - accuracy: 0.9201\n",
            "Epoch 19: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 19: accuracy did not improve from 0.94580\n",
            "78/78 [==============================] - 62s 792ms/step - loss: 0.5715 - accuracy: 0.9201 - val_loss: 1.9821 - val_accuracy: 0.8358\n",
            "Epoch 20/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.9117\n",
            "Epoch 20: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 20: accuracy did not improve from 0.94580\n",
            "78/78 [==============================] - 60s 774ms/step - loss: 0.6929 - accuracy: 0.9117 - val_loss: 1.7910 - val_accuracy: 0.8652\n",
            "Epoch 21/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4448 - accuracy: 0.9410\n",
            "Epoch 21: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 21: accuracy did not improve from 0.94580\n",
            "78/78 [==============================] - 62s 799ms/step - loss: 0.4448 - accuracy: 0.9410 - val_loss: 1.7145 - val_accuracy: 0.8709\n",
            "Epoch 22/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5276 - accuracy: 0.9358\n",
            "Epoch 22: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 22: accuracy did not improve from 0.94580\n",
            "78/78 [==============================] - 62s 799ms/step - loss: 0.5276 - accuracy: 0.9358 - val_loss: 3.0868 - val_accuracy: 0.8177\n",
            "Epoch 23/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5321 - accuracy: 0.9350\n",
            "Epoch 23: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 23: accuracy did not improve from 0.94580\n",
            "78/78 [==============================] - 60s 772ms/step - loss: 0.5321 - accuracy: 0.9350 - val_loss: 2.6739 - val_accuracy: 0.8313\n",
            "Epoch 24/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5712 - accuracy: 0.9301\n",
            "Epoch 24: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 24: accuracy did not improve from 0.94580\n",
            "78/78 [==============================] - 62s 803ms/step - loss: 0.5712 - accuracy: 0.9301 - val_loss: 2.1463 - val_accuracy: 0.8607\n",
            "Epoch 25/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7340 - accuracy: 0.9181\n",
            "Epoch 25: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 25: accuracy did not improve from 0.94580\n",
            "78/78 [==============================] - 62s 801ms/step - loss: 0.7340 - accuracy: 0.9181 - val_loss: 2.1492 - val_accuracy: 0.8618\n",
            "Epoch 26/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.9502\n",
            "Epoch 26: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 26: accuracy improved from 0.94580 to 0.95022, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 61s 781ms/step - loss: 0.3726 - accuracy: 0.9502 - val_loss: 2.3012 - val_accuracy: 0.8482\n",
            "Epoch 27/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3538 - accuracy: 0.9558\n",
            "Epoch 27: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 27: accuracy improved from 0.95022 to 0.95584, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 62s 789ms/step - loss: 0.3538 - accuracy: 0.9558 - val_loss: 2.2546 - val_accuracy: 0.8686\n",
            "Epoch 28/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.9506\n",
            "Epoch 28: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 28: accuracy did not improve from 0.95584\n",
            "78/78 [==============================] - 61s 783ms/step - loss: 0.3876 - accuracy: 0.9506 - val_loss: 2.4839 - val_accuracy: 0.8482\n",
            "Epoch 29/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4019 - accuracy: 0.9518\n",
            "Epoch 29: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 29: accuracy did not improve from 0.95584\n",
            "78/78 [==============================] - 61s 785ms/step - loss: 0.4019 - accuracy: 0.9518 - val_loss: 1.9932 - val_accuracy: 0.8652\n",
            "Epoch 30/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6302 - accuracy: 0.9310\n",
            "Epoch 30: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 30: accuracy did not improve from 0.95584\n",
            "78/78 [==============================] - 60s 769ms/step - loss: 0.6302 - accuracy: 0.9310 - val_loss: 2.0314 - val_accuracy: 0.8664\n",
            "Epoch 31/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.9570\n",
            "Epoch 31: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 31: accuracy improved from 0.95584 to 0.95705, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 811ms/step - loss: 0.3200 - accuracy: 0.9570 - val_loss: 1.9321 - val_accuracy: 0.8732\n",
            "Epoch 32/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.9502\n",
            "Epoch 32: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 32: accuracy did not improve from 0.95705\n",
            "78/78 [==============================] - 61s 779ms/step - loss: 0.4241 - accuracy: 0.9502 - val_loss: 2.4528 - val_accuracy: 0.8607\n",
            "Epoch 33/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6119 - accuracy: 0.9374\n",
            "Epoch 33: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 33: accuracy did not improve from 0.95705\n",
            "78/78 [==============================] - 60s 775ms/step - loss: 0.6119 - accuracy: 0.9374 - val_loss: 1.8848 - val_accuracy: 0.8811\n",
            "Epoch 34/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.9514\n",
            "Epoch 34: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 34: accuracy did not improve from 0.95705\n",
            "78/78 [==============================] - 61s 789ms/step - loss: 0.4669 - accuracy: 0.9514 - val_loss: 1.8587 - val_accuracy: 0.8777\n",
            "Epoch 35/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4443 - accuracy: 0.9474\n",
            "Epoch 35: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 35: accuracy did not improve from 0.95705\n",
            "78/78 [==============================] - 61s 784ms/step - loss: 0.4443 - accuracy: 0.9474 - val_loss: 2.0481 - val_accuracy: 0.8698\n",
            "Epoch 36/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5118 - accuracy: 0.9486\n",
            "Epoch 36: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 36: accuracy did not improve from 0.95705\n",
            "78/78 [==============================] - 61s 774ms/step - loss: 0.5118 - accuracy: 0.9486 - val_loss: 3.3439 - val_accuracy: 0.8120\n",
            "Epoch 37/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.9430\n",
            "Epoch 37: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 37: accuracy did not improve from 0.95705\n",
            "78/78 [==============================] - 60s 775ms/step - loss: 0.4976 - accuracy: 0.9430 - val_loss: 2.7526 - val_accuracy: 0.8562\n",
            "Epoch 38/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4034 - accuracy: 0.9526\n",
            "Epoch 38: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 38: accuracy did not improve from 0.95705\n",
            "78/78 [==============================] - 61s 783ms/step - loss: 0.4034 - accuracy: 0.9526 - val_loss: 2.5919 - val_accuracy: 0.8743\n",
            "Epoch 39/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3403 - accuracy: 0.9578\n",
            "Epoch 39: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 39: accuracy improved from 0.95705 to 0.95785, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 61s 787ms/step - loss: 0.3403 - accuracy: 0.9578 - val_loss: 2.2677 - val_accuracy: 0.8766\n",
            "Epoch 40/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.9506\n",
            "Epoch 40: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 40: accuracy did not improve from 0.95785\n",
            "78/78 [==============================] - 61s 785ms/step - loss: 0.4952 - accuracy: 0.9506 - val_loss: 2.3108 - val_accuracy: 0.8777\n",
            "Epoch 41/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.9587\n",
            "Epoch 41: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 41: accuracy improved from 0.95785 to 0.95865, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 62s 802ms/step - loss: 0.4107 - accuracy: 0.9587 - val_loss: 2.8101 - val_accuracy: 0.8675\n",
            "Epoch 42/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4222 - accuracy: 0.9498\n",
            "Epoch 42: val_accuracy did not improve from 0.88335\n",
            "\n",
            "Epoch 42: accuracy did not improve from 0.95865\n",
            "78/78 [==============================] - 61s 784ms/step - loss: 0.4222 - accuracy: 0.9498 - val_loss: 2.6531 - val_accuracy: 0.8766\n",
            "Epoch 43/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4662 - accuracy: 0.9534\n",
            "Epoch 43: val_accuracy improved from 0.88335 to 0.89354, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5\n",
            "\n",
            "Epoch 43: accuracy did not improve from 0.95865\n",
            "78/78 [==============================] - 63s 813ms/step - loss: 0.4662 - accuracy: 0.9534 - val_loss: 2.1045 - val_accuracy: 0.8935\n",
            "Epoch 44/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.9627\n",
            "Epoch 44: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 44: accuracy improved from 0.95865 to 0.96267, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 62s 792ms/step - loss: 0.4117 - accuracy: 0.9627 - val_loss: 3.4985 - val_accuracy: 0.8165\n",
            "Epoch 45/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4170 - accuracy: 0.9595\n",
            "Epoch 45: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 45: accuracy did not improve from 0.96267\n",
            "78/78 [==============================] - 62s 802ms/step - loss: 0.4170 - accuracy: 0.9595 - val_loss: 2.2919 - val_accuracy: 0.8675\n",
            "Epoch 46/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4789 - accuracy: 0.9510\n",
            "Epoch 46: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 46: accuracy did not improve from 0.96267\n",
            "78/78 [==============================] - 60s 767ms/step - loss: 0.4789 - accuracy: 0.9510 - val_loss: 2.3585 - val_accuracy: 0.8709\n",
            "Epoch 47/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3871 - accuracy: 0.9603\n",
            "Epoch 47: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 47: accuracy did not improve from 0.96267\n",
            "78/78 [==============================] - 62s 799ms/step - loss: 0.3871 - accuracy: 0.9603 - val_loss: 3.9734 - val_accuracy: 0.8279\n",
            "Epoch 48/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4937 - accuracy: 0.9530\n",
            "Epoch 48: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 48: accuracy did not improve from 0.96267\n",
            "78/78 [==============================] - 62s 801ms/step - loss: 0.4937 - accuracy: 0.9530 - val_loss: 3.6201 - val_accuracy: 0.8471\n",
            "Epoch 49/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4227 - accuracy: 0.9639\n",
            "Epoch 49: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 49: accuracy improved from 0.96267 to 0.96387, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 61s 786ms/step - loss: 0.4227 - accuracy: 0.9639 - val_loss: 2.5012 - val_accuracy: 0.8890\n",
            "Epoch 50/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.9570\n",
            "Epoch 50: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 50: accuracy did not improve from 0.96387\n",
            "78/78 [==============================] - 61s 781ms/step - loss: 0.4743 - accuracy: 0.9570 - val_loss: 7.1334 - val_accuracy: 0.7712\n",
            "Epoch 51/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.9562\n",
            "Epoch 51: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 51: accuracy did not improve from 0.96387\n",
            "78/78 [==============================] - 61s 775ms/step - loss: 0.5274 - accuracy: 0.9562 - val_loss: 2.1815 - val_accuracy: 0.8901\n",
            "Epoch 52/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3846 - accuracy: 0.9675\n",
            "Epoch 52: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 52: accuracy improved from 0.96387 to 0.96748, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 61s 790ms/step - loss: 0.3846 - accuracy: 0.9675 - val_loss: 3.2328 - val_accuracy: 0.8618\n",
            "Epoch 53/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.9687\n",
            "Epoch 53: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 53: accuracy improved from 0.96748 to 0.96869, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 62s 801ms/step - loss: 0.3355 - accuracy: 0.9687 - val_loss: 2.4650 - val_accuracy: 0.8800\n",
            "Epoch 54/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.9599\n",
            "Epoch 54: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 54: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 61s 783ms/step - loss: 0.4303 - accuracy: 0.9599 - val_loss: 3.0186 - val_accuracy: 0.8494\n",
            "Epoch 55/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.9647\n",
            "Epoch 55: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 55: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 61s 777ms/step - loss: 0.4431 - accuracy: 0.9647 - val_loss: 2.8915 - val_accuracy: 0.8664\n",
            "Epoch 56/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.9582\n",
            "Epoch 56: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 56: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 60s 773ms/step - loss: 0.4303 - accuracy: 0.9582 - val_loss: 2.4272 - val_accuracy: 0.8890\n",
            "Epoch 57/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.9683\n",
            "Epoch 57: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 57: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 62s 793ms/step - loss: 0.3521 - accuracy: 0.9683 - val_loss: 2.9740 - val_accuracy: 0.8743\n",
            "Epoch 58/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.9486\n",
            "Epoch 58: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 58: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 60s 767ms/step - loss: 0.5656 - accuracy: 0.9486 - val_loss: 3.1911 - val_accuracy: 0.8732\n",
            "Epoch 59/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.9651\n",
            "Epoch 59: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 59: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 61s 776ms/step - loss: 0.3970 - accuracy: 0.9651 - val_loss: 2.9534 - val_accuracy: 0.8800\n",
            "Epoch 60/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3938 - accuracy: 0.9651\n",
            "Epoch 60: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 60: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 62s 790ms/step - loss: 0.3938 - accuracy: 0.9651 - val_loss: 3.0314 - val_accuracy: 0.8686\n",
            "Epoch 61/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4332 - accuracy: 0.9578\n",
            "Epoch 61: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 61: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 60s 767ms/step - loss: 0.4332 - accuracy: 0.9578 - val_loss: 3.1648 - val_accuracy: 0.8641\n",
            "Epoch 62/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.9627\n",
            "Epoch 62: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 62: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 60s 771ms/step - loss: 0.4474 - accuracy: 0.9627 - val_loss: 3.8768 - val_accuracy: 0.8550\n",
            "Epoch 63/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4024 - accuracy: 0.9639\n",
            "Epoch 63: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 63: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 62s 795ms/step - loss: 0.4024 - accuracy: 0.9639 - val_loss: 2.8609 - val_accuracy: 0.8822\n",
            "Epoch 64/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.9639\n",
            "Epoch 64: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 64: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 61s 784ms/step - loss: 0.4489 - accuracy: 0.9639 - val_loss: 2.9427 - val_accuracy: 0.8901\n",
            "Epoch 65/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4076 - accuracy: 0.9647\n",
            "Epoch 65: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 65: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 61s 784ms/step - loss: 0.4076 - accuracy: 0.9647 - val_loss: 4.0469 - val_accuracy: 0.8494\n",
            "Epoch 66/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5502 - accuracy: 0.9562\n",
            "Epoch 66: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 66: accuracy did not improve from 0.96869\n",
            "78/78 [==============================] - 61s 780ms/step - loss: 0.5502 - accuracy: 0.9562 - val_loss: 2.8722 - val_accuracy: 0.8879\n",
            "Epoch 67/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9767\n",
            "Epoch 67: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 67: accuracy improved from 0.96869 to 0.97672, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 62s 797ms/step - loss: 0.2626 - accuracy: 0.9767 - val_loss: 3.2581 - val_accuracy: 0.8822\n",
            "Epoch 68/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.9691\n",
            "Epoch 68: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 68: accuracy did not improve from 0.97672\n",
            "78/78 [==============================] - 61s 777ms/step - loss: 0.3312 - accuracy: 0.9691 - val_loss: 2.8739 - val_accuracy: 0.8901\n",
            "Epoch 69/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3716 - accuracy: 0.9703\n",
            "Epoch 69: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 69: accuracy did not improve from 0.97672\n",
            "78/78 [==============================] - 61s 773ms/step - loss: 0.3716 - accuracy: 0.9703 - val_loss: 3.3436 - val_accuracy: 0.8743\n",
            "Epoch 70/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.9675\n",
            "Epoch 70: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 70: accuracy did not improve from 0.97672\n",
            "78/78 [==============================] - 61s 776ms/step - loss: 0.3622 - accuracy: 0.9675 - val_loss: 3.0339 - val_accuracy: 0.8924\n",
            "Epoch 71/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2837 - accuracy: 0.9727\n",
            "Epoch 71: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 71: accuracy did not improve from 0.97672\n",
            "78/78 [==============================] - 60s 776ms/step - loss: 0.2837 - accuracy: 0.9727 - val_loss: 6.5527 - val_accuracy: 0.8143\n",
            "Epoch 72/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9050 - accuracy: 0.9414\n",
            "Epoch 72: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 72: accuracy did not improve from 0.97672\n",
            "78/78 [==============================] - 60s 775ms/step - loss: 0.9050 - accuracy: 0.9414 - val_loss: 4.2889 - val_accuracy: 0.8607\n",
            "Epoch 73/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7086 - accuracy: 0.9510\n",
            "Epoch 73: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 73: accuracy did not improve from 0.97672\n",
            "78/78 [==============================] - 61s 778ms/step - loss: 0.7086 - accuracy: 0.9510 - val_loss: 2.8202 - val_accuracy: 0.8777\n",
            "Epoch 74/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9795\n",
            "Epoch 74: val_accuracy did not improve from 0.89354\n",
            "\n",
            "Epoch 74: accuracy improved from 0.97672 to 0.97953, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 61s 782ms/step - loss: 0.2306 - accuracy: 0.9795 - val_loss: 4.9264 - val_accuracy: 0.8471\n",
            "Epoch 75/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.9759\n",
            "Epoch 75: val_accuracy improved from 0.89354 to 0.90147, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5\n",
            "\n",
            "Epoch 75: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 63s 815ms/step - loss: 0.2831 - accuracy: 0.9759 - val_loss: 2.6795 - val_accuracy: 0.9015\n",
            "Epoch 76/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9783\n",
            "Epoch 76: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 76: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 770ms/step - loss: 0.2448 - accuracy: 0.9783 - val_loss: 3.2986 - val_accuracy: 0.8867\n",
            "Epoch 77/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4790 - accuracy: 0.9631\n",
            "Epoch 77: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 77: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 769ms/step - loss: 0.4790 - accuracy: 0.9631 - val_loss: 3.6004 - val_accuracy: 0.8822\n",
            "Epoch 78/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3993 - accuracy: 0.9691\n",
            "Epoch 78: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 78: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 61s 784ms/step - loss: 0.3993 - accuracy: 0.9691 - val_loss: 3.4943 - val_accuracy: 0.8845\n",
            "Epoch 79/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9771\n",
            "Epoch 79: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 79: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 771ms/step - loss: 0.2413 - accuracy: 0.9771 - val_loss: 2.9785 - val_accuracy: 0.8924\n",
            "Epoch 80/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.9659\n",
            "Epoch 80: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 80: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 771ms/step - loss: 0.4331 - accuracy: 0.9659 - val_loss: 3.6144 - val_accuracy: 0.8822\n",
            "Epoch 81/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3835 - accuracy: 0.9703\n",
            "Epoch 81: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 81: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 61s 782ms/step - loss: 0.3835 - accuracy: 0.9703 - val_loss: 3.8807 - val_accuracy: 0.8573\n",
            "Epoch 82/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.9691\n",
            "Epoch 82: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 82: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 774ms/step - loss: 0.3078 - accuracy: 0.9691 - val_loss: 3.9962 - val_accuracy: 0.8732\n",
            "Epoch 83/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3255 - accuracy: 0.9743\n",
            "Epoch 83: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 83: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 62s 797ms/step - loss: 0.3255 - accuracy: 0.9743 - val_loss: 2.6663 - val_accuracy: 0.8947\n",
            "Epoch 84/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.9691\n",
            "Epoch 84: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 84: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 768ms/step - loss: 0.3198 - accuracy: 0.9691 - val_loss: 3.9047 - val_accuracy: 0.8664\n",
            "Epoch 85/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5968 - accuracy: 0.9619\n",
            "Epoch 85: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 85: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 765ms/step - loss: 0.5968 - accuracy: 0.9619 - val_loss: 3.3370 - val_accuracy: 0.8856\n",
            "Epoch 86/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3153 - accuracy: 0.9759\n",
            "Epoch 86: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 86: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 61s 778ms/step - loss: 0.3153 - accuracy: 0.9759 - val_loss: 3.3945 - val_accuracy: 0.8867\n",
            "Epoch 87/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9779\n",
            "Epoch 87: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 87: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 61s 780ms/step - loss: 0.2454 - accuracy: 0.9779 - val_loss: 3.2739 - val_accuracy: 0.8834\n",
            "Epoch 88/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3457 - accuracy: 0.9707\n",
            "Epoch 88: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 88: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 771ms/step - loss: 0.3457 - accuracy: 0.9707 - val_loss: 3.1609 - val_accuracy: 0.9003\n",
            "Epoch 89/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3112 - accuracy: 0.9691\n",
            "Epoch 89: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 89: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 61s 778ms/step - loss: 0.3112 - accuracy: 0.9691 - val_loss: 4.7128 - val_accuracy: 0.8528\n",
            "Epoch 90/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.9735\n",
            "Epoch 90: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 90: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 770ms/step - loss: 0.3306 - accuracy: 0.9735 - val_loss: 4.4848 - val_accuracy: 0.8664\n",
            "Epoch 91/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3522 - accuracy: 0.9771\n",
            "Epoch 91: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 91: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 60s 776ms/step - loss: 0.3522 - accuracy: 0.9771 - val_loss: 3.4273 - val_accuracy: 0.8890\n",
            "Epoch 92/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3352 - accuracy: 0.9699\n",
            "Epoch 92: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 92: accuracy did not improve from 0.97953\n",
            "78/78 [==============================] - 62s 799ms/step - loss: 0.3352 - accuracy: 0.9699 - val_loss: 3.4970 - val_accuracy: 0.8913\n",
            "Epoch 93/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9827\n",
            "Epoch 93: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 93: accuracy improved from 0.97953 to 0.98274, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 814ms/step - loss: 0.1702 - accuracy: 0.9827 - val_loss: 4.2075 - val_accuracy: 0.8607\n",
            "Epoch 94/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4788 - accuracy: 0.9679\n",
            "Epoch 94: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 94: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 773ms/step - loss: 0.4788 - accuracy: 0.9679 - val_loss: 3.3567 - val_accuracy: 0.8947\n",
            "Epoch 95/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4126 - accuracy: 0.9695\n",
            "Epoch 95: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 95: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 62s 794ms/step - loss: 0.4126 - accuracy: 0.9695 - val_loss: 3.3719 - val_accuracy: 0.8777\n",
            "Epoch 96/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4063 - accuracy: 0.9723\n",
            "Epoch 96: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 96: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 770ms/step - loss: 0.4063 - accuracy: 0.9723 - val_loss: 3.6927 - val_accuracy: 0.8777\n",
            "Epoch 97/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.1636 - accuracy: 0.9799\n",
            "Epoch 97: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 97: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 62s 801ms/step - loss: 0.1636 - accuracy: 0.9799 - val_loss: 2.8150 - val_accuracy: 0.8992\n",
            "Epoch 98/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.9819\n",
            "Epoch 98: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 98: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 61s 775ms/step - loss: 0.2798 - accuracy: 0.9819 - val_loss: 3.4285 - val_accuracy: 0.8924\n",
            "Epoch 99/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4333 - accuracy: 0.9731\n",
            "Epoch 99: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 99: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 775ms/step - loss: 0.4333 - accuracy: 0.9731 - val_loss: 5.9807 - val_accuracy: 0.8414\n",
            "Epoch 100/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3729 - accuracy: 0.9699\n",
            "Epoch 100: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 100: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 61s 782ms/step - loss: 0.3729 - accuracy: 0.9699 - val_loss: 4.3678 - val_accuracy: 0.8675\n",
            "Epoch 101/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.9699\n",
            "Epoch 101: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 101: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 61s 785ms/step - loss: 0.3828 - accuracy: 0.9699 - val_loss: 5.6564 - val_accuracy: 0.8448\n",
            "Epoch 102/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.9683\n",
            "Epoch 102: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 102: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 61s 773ms/step - loss: 0.4465 - accuracy: 0.9683 - val_loss: 3.7144 - val_accuracy: 0.8641\n",
            "Epoch 103/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.9767\n",
            "Epoch 103: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 103: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 762ms/step - loss: 0.3365 - accuracy: 0.9767 - val_loss: 3.1682 - val_accuracy: 0.8834\n",
            "Epoch 104/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4805 - accuracy: 0.9703\n",
            "Epoch 104: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 104: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 771ms/step - loss: 0.4805 - accuracy: 0.9703 - val_loss: 3.5722 - val_accuracy: 0.8788\n",
            "Epoch 105/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9364 - accuracy: 0.9482\n",
            "Epoch 105: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 105: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 62s 793ms/step - loss: 0.9364 - accuracy: 0.9482 - val_loss: 4.3102 - val_accuracy: 0.8482\n",
            "Epoch 106/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4476 - accuracy: 0.9683\n",
            "Epoch 106: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 106: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 61s 783ms/step - loss: 0.4476 - accuracy: 0.9683 - val_loss: 4.5035 - val_accuracy: 0.8664\n",
            "Epoch 107/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.9759\n",
            "Epoch 107: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 107: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 61s 779ms/step - loss: 0.3898 - accuracy: 0.9759 - val_loss: 3.5765 - val_accuracy: 0.8788\n",
            "Epoch 108/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4406 - accuracy: 0.9719\n",
            "Epoch 108: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 108: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 765ms/step - loss: 0.4406 - accuracy: 0.9719 - val_loss: 3.1404 - val_accuracy: 0.8901\n",
            "Epoch 109/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4374 - accuracy: 0.9715\n",
            "Epoch 109: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 109: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 772ms/step - loss: 0.4374 - accuracy: 0.9715 - val_loss: 3.7029 - val_accuracy: 0.8822\n",
            "Epoch 110/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2870 - accuracy: 0.9799\n",
            "Epoch 110: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 110: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 777ms/step - loss: 0.2870 - accuracy: 0.9799 - val_loss: 5.7813 - val_accuracy: 0.8460\n",
            "Epoch 111/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.9783\n",
            "Epoch 111: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 111: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 61s 782ms/step - loss: 0.3176 - accuracy: 0.9783 - val_loss: 4.0631 - val_accuracy: 0.8867\n",
            "Epoch 112/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4333 - accuracy: 0.9691\n",
            "Epoch 112: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 112: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 61s 780ms/step - loss: 0.4333 - accuracy: 0.9691 - val_loss: 4.9960 - val_accuracy: 0.8528\n",
            "Epoch 113/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.9671\n",
            "Epoch 113: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 113: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 768ms/step - loss: 0.4791 - accuracy: 0.9671 - val_loss: 6.0629 - val_accuracy: 0.8358\n",
            "Epoch 114/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4452 - accuracy: 0.9707\n",
            "Epoch 114: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 114: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 771ms/step - loss: 0.4452 - accuracy: 0.9707 - val_loss: 3.1314 - val_accuracy: 0.8924\n",
            "Epoch 115/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.9775\n",
            "Epoch 115: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 115: accuracy did not improve from 0.98274\n",
            "78/78 [==============================] - 60s 775ms/step - loss: 0.3291 - accuracy: 0.9775 - val_loss: 3.9917 - val_accuracy: 0.8811\n",
            "Epoch 116/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9851\n",
            "Epoch 116: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 116: accuracy improved from 0.98274 to 0.98515, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 61s 781ms/step - loss: 0.1540 - accuracy: 0.9851 - val_loss: 3.4987 - val_accuracy: 0.8901\n",
            "Epoch 117/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.9827\n",
            "Epoch 117: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 117: accuracy did not improve from 0.98515\n",
            "78/78 [==============================] - 62s 799ms/step - loss: 0.2705 - accuracy: 0.9827 - val_loss: 3.8951 - val_accuracy: 0.8777\n",
            "Epoch 118/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3038 - accuracy: 0.9827\n",
            "Epoch 118: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 118: accuracy did not improve from 0.98515\n",
            "78/78 [==============================] - 62s 790ms/step - loss: 0.3038 - accuracy: 0.9827 - val_loss: 4.0968 - val_accuracy: 0.8777\n",
            "Epoch 119/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.9803\n",
            "Epoch 119: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 119: accuracy did not improve from 0.98515\n",
            "78/78 [==============================] - 60s 769ms/step - loss: 0.3045 - accuracy: 0.9803 - val_loss: 3.0982 - val_accuracy: 0.8879\n",
            "Epoch 120/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2995 - accuracy: 0.9787\n",
            "Epoch 120: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 120: accuracy did not improve from 0.98515\n",
            "78/78 [==============================] - 60s 768ms/step - loss: 0.2995 - accuracy: 0.9787 - val_loss: 3.9571 - val_accuracy: 0.8766\n",
            "Epoch 121/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 0.9864\n",
            "Epoch 121: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 121: accuracy improved from 0.98515 to 0.98635, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 809ms/step - loss: 0.2207 - accuracy: 0.9864 - val_loss: 3.9262 - val_accuracy: 0.8822\n",
            "Epoch 122/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.9659\n",
            "Epoch 122: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 122: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 769ms/step - loss: 0.5223 - accuracy: 0.9659 - val_loss: 4.3209 - val_accuracy: 0.8777\n",
            "Epoch 123/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4863 - accuracy: 0.9759\n",
            "Epoch 123: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 123: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 62s 792ms/step - loss: 0.4863 - accuracy: 0.9759 - val_loss: 4.1139 - val_accuracy: 0.8800\n",
            "Epoch 124/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.9731\n",
            "Epoch 124: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 124: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 768ms/step - loss: 0.4331 - accuracy: 0.9731 - val_loss: 4.6494 - val_accuracy: 0.8641\n",
            "Epoch 125/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9763\n",
            "Epoch 125: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 125: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 769ms/step - loss: 0.2755 - accuracy: 0.9763 - val_loss: 4.9957 - val_accuracy: 0.8528\n",
            "Epoch 126/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.9791\n",
            "Epoch 126: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 126: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 776ms/step - loss: 0.3270 - accuracy: 0.9791 - val_loss: 5.3973 - val_accuracy: 0.8652\n",
            "Epoch 127/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3743 - accuracy: 0.9779\n",
            "Epoch 127: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 127: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 62s 794ms/step - loss: 0.3743 - accuracy: 0.9779 - val_loss: 3.6093 - val_accuracy: 0.8890\n",
            "Epoch 128/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.9795\n",
            "Epoch 128: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 128: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 768ms/step - loss: 0.2984 - accuracy: 0.9795 - val_loss: 4.7945 - val_accuracy: 0.8732\n",
            "Epoch 129/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.9795\n",
            "Epoch 129: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 129: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 61s 780ms/step - loss: 0.3299 - accuracy: 0.9795 - val_loss: 5.4995 - val_accuracy: 0.8550\n",
            "Epoch 130/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.9803\n",
            "Epoch 130: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 130: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 62s 795ms/step - loss: 0.2603 - accuracy: 0.9803 - val_loss: 4.2848 - val_accuracy: 0.8686\n",
            "Epoch 131/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.9799\n",
            "Epoch 131: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 131: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 768ms/step - loss: 0.3000 - accuracy: 0.9799 - val_loss: 4.2838 - val_accuracy: 0.8834\n",
            "Epoch 132/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9823\n",
            "Epoch 132: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 132: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 62s 794ms/step - loss: 0.2048 - accuracy: 0.9823 - val_loss: 4.5622 - val_accuracy: 0.8641\n",
            "Epoch 133/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.9759\n",
            "Epoch 133: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 133: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 767ms/step - loss: 0.3691 - accuracy: 0.9759 - val_loss: 4.3540 - val_accuracy: 0.8800\n",
            "Epoch 134/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.9823\n",
            "Epoch 134: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 134: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 769ms/step - loss: 0.2424 - accuracy: 0.9823 - val_loss: 4.4010 - val_accuracy: 0.8777\n",
            "Epoch 135/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9819\n",
            "Epoch 135: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 135: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 61s 783ms/step - loss: 0.2852 - accuracy: 0.9819 - val_loss: 6.6594 - val_accuracy: 0.8516\n",
            "Epoch 136/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.9807\n",
            "Epoch 136: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 136: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 61s 786ms/step - loss: 0.2873 - accuracy: 0.9807 - val_loss: 4.8615 - val_accuracy: 0.8777\n",
            "Epoch 137/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.9787\n",
            "Epoch 137: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 137: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 61s 779ms/step - loss: 0.3855 - accuracy: 0.9787 - val_loss: 5.9298 - val_accuracy: 0.8630\n",
            "Epoch 138/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.9759\n",
            "Epoch 138: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 138: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 772ms/step - loss: 0.4119 - accuracy: 0.9759 - val_loss: 4.5546 - val_accuracy: 0.8845\n",
            "Epoch 139/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3582 - accuracy: 0.9799\n",
            "Epoch 139: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 139: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 764ms/step - loss: 0.3582 - accuracy: 0.9799 - val_loss: 4.7641 - val_accuracy: 0.8550\n",
            "Epoch 140/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.9731\n",
            "Epoch 140: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 140: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 62s 796ms/step - loss: 0.4469 - accuracy: 0.9731 - val_loss: 4.1708 - val_accuracy: 0.8856\n",
            "Epoch 141/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.9771\n",
            "Epoch 141: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 141: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 61s 781ms/step - loss: 0.3662 - accuracy: 0.9771 - val_loss: 4.8550 - val_accuracy: 0.8777\n",
            "Epoch 142/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.9835\n",
            "Epoch 142: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 142: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 770ms/step - loss: 0.2354 - accuracy: 0.9835 - val_loss: 5.8138 - val_accuracy: 0.8630\n",
            "Epoch 143/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.9727\n",
            "Epoch 143: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 143: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 61s 790ms/step - loss: 0.4128 - accuracy: 0.9727 - val_loss: 5.0473 - val_accuracy: 0.8777\n",
            "Epoch 144/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4044 - accuracy: 0.9751\n",
            "Epoch 144: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 144: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 769ms/step - loss: 0.4044 - accuracy: 0.9751 - val_loss: 6.2106 - val_accuracy: 0.8584\n",
            "Epoch 145/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4027 - accuracy: 0.9763\n",
            "Epoch 145: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 145: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 62s 791ms/step - loss: 0.4027 - accuracy: 0.9763 - val_loss: 4.4404 - val_accuracy: 0.8788\n",
            "Epoch 146/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9815\n",
            "Epoch 146: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 146: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 61s 787ms/step - loss: 0.2250 - accuracy: 0.9815 - val_loss: 4.1924 - val_accuracy: 0.8913\n",
            "Epoch 147/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.9807\n",
            "Epoch 147: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 147: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 765ms/step - loss: 0.2957 - accuracy: 0.9807 - val_loss: 4.6409 - val_accuracy: 0.8856\n",
            "Epoch 148/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.9835\n",
            "Epoch 148: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 148: accuracy did not improve from 0.98635\n",
            "78/78 [==============================] - 60s 769ms/step - loss: 0.2999 - accuracy: 0.9835 - val_loss: 5.0367 - val_accuracy: 0.8845\n",
            "Epoch 149/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9880\n",
            "Epoch 149: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 149: accuracy improved from 0.98635 to 0.98796, saving model to /content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\n",
            "78/78 [==============================] - 63s 808ms/step - loss: 0.2005 - accuracy: 0.9880 - val_loss: 6.0062 - val_accuracy: 0.8686\n",
            "Epoch 150/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2246 - accuracy: 0.9835\n",
            "Epoch 150: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 150: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 60s 771ms/step - loss: 0.2246 - accuracy: 0.9835 - val_loss: 7.1802 - val_accuracy: 0.8414\n",
            "Epoch 151/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.9795\n",
            "Epoch 151: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 151: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 62s 794ms/step - loss: 0.3319 - accuracy: 0.9795 - val_loss: 6.4253 - val_accuracy: 0.8539\n",
            "Epoch 152/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.9775\n",
            "Epoch 152: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 152: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 60s 766ms/step - loss: 0.3102 - accuracy: 0.9775 - val_loss: 6.8241 - val_accuracy: 0.8448\n",
            "Epoch 153/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3159 - accuracy: 0.9787\n",
            "Epoch 153: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 153: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 60s 767ms/step - loss: 0.3159 - accuracy: 0.9787 - val_loss: 4.6534 - val_accuracy: 0.8743\n",
            "Epoch 154/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.9831\n",
            "Epoch 154: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 154: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 61s 781ms/step - loss: 0.2568 - accuracy: 0.9831 - val_loss: 5.9804 - val_accuracy: 0.8664\n",
            "Epoch 155/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.9767\n",
            "Epoch 155: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 155: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 60s 767ms/step - loss: 0.4535 - accuracy: 0.9767 - val_loss: 5.1378 - val_accuracy: 0.8777\n",
            "Epoch 156/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3738 - accuracy: 0.9811\n",
            "Epoch 156: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 156: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 60s 771ms/step - loss: 0.3738 - accuracy: 0.9811 - val_loss: 5.9347 - val_accuracy: 0.8675\n",
            "Epoch 157/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3730 - accuracy: 0.9747\n",
            "Epoch 157: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 157: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 62s 790ms/step - loss: 0.3730 - accuracy: 0.9747 - val_loss: 4.5040 - val_accuracy: 0.8867\n",
            "Epoch 158/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.9799\n",
            "Epoch 158: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 158: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 61s 789ms/step - loss: 0.3217 - accuracy: 0.9799 - val_loss: 5.7729 - val_accuracy: 0.8562\n",
            "Epoch 159/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.9779\n",
            "Epoch 159: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 159: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 61s 781ms/step - loss: 0.3807 - accuracy: 0.9779 - val_loss: 5.9642 - val_accuracy: 0.8664\n",
            "Epoch 160/160\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.9835\n",
            "Epoch 160: val_accuracy did not improve from 0.90147\n",
            "\n",
            "Epoch 160: accuracy did not improve from 0.98796\n",
            "78/78 [==============================] - 61s 784ms/step - loss: 0.3104 - accuracy: 0.9835 - val_loss: 5.7079 - val_accuracy: 0.8596\n"
          ]
        }
      ],
      "source": [
        "filepath = \"/content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5\"\n",
        "filepath2 = \"/content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_train2.h5\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,\n",
        "                             save_best_only=True, mode='max')\n",
        "checkpoint2 = ModelCheckpoint(filepath2, monitor='accuracy', verbose=1,\n",
        "                             save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1,checkpoint2]\n",
        "r = model.fit_generator(\n",
        "    training_set,\n",
        "    epochs=160,\n",
        "    validation_data=val_set,\n",
        "    steps_per_epoch=len(training_set),\n",
        "    validation_steps=len(val_set),\n",
        "    callbacks=callbacks_list\n",
        ")\n",
        "model.save_weights(\"/content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/end2.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "mgiXmC4MO6y8",
        "outputId": "f37faed8-06a8-45f6-efad-a54c90b4f2d2"
      },
      "outputs": [],
      "source": [
        "#plot of accuracy and loss\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "# plot the loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.title('Training and validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "# plot the accuracy\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbklX5d7O610",
        "outputId": "471bb832-7761-4719-866a-45a62b6d8c19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-29eded4fc0ae>:4: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  preds = model.evaluate_generator(test_set)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss = 2.266108751296997\n",
            "Test Accuracy = 0.9002217054367065\n"
          ]
        }
      ],
      "source": [
        "#evaluating the model (test acc)\n",
        "#batch size = 32\n",
        "model.load_weights('/content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5')\n",
        "preds = model.evaluate_generator(test_set)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZqoNErNCO64r",
        "outputId": "9ac64aa2-fd13-44d5-ef68-a6b5846b6994"
      },
      "outputs": [],
      "source": [
        "#confusion matrix\n",
        "\n",
        "#you have to set test bath size=1 before running the cell\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import tensorflow as tf\n",
        "model.load_weights('/content/drive/MyDrive/Research/defense/Accuracy/InceptionV3/highest_val2.h5')\n",
        "filenames=test_set.filenames\n",
        "nb_samples=len(test_set)\n",
        "y_prob=[]\n",
        "y_act=[]\n",
        "test_set.reset()\n",
        "for _ in range (nb_samples):\n",
        "    X_test,Y_test = test_set.next()\n",
        "    y_prob.append(model.predict(X_test))\n",
        "    y_act.append(Y_test)\n",
        "predicted_class=[list(training_set.class_indices.keys())[i.argmax()] for i in y_prob]\n",
        "actual_class=[list(training_set.class_indices.keys())[i.argmax()]for i in y_act]\n",
        "out_df=pd.DataFrame(np.vstack([predicted_class,actual_class]).T,columns=['predicted_class','actual_class'])\n",
        "confusion_matrix=pd.crosstab(out_df['actual_class'],out_df['predicted_class'],rownames=['Actual'],colnames=['Predicted'])\n",
        "import matplotlib.pyplot as plt\n",
        "sn.heatmap(confusion_matrix,cmap='flare', annot=True, fmt='d')\n",
        "plt.show()\n",
        "#plt.savefig('/content/drive/MyDrive/model weights/vgg16_AugGfb_split1_maxval_3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxcsBGNODQVm"
      },
      "outputs": [],
      "source": [
        "lst=[r.history['loss'],r.history['val_loss'],r.history['accuracy'],r.history['val_accuracy']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "vEFhrPlrDQtc",
        "outputId": "924cb543-ece1-44a2-c3b6-4906784ef377"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b38c18be-ce71-42ca-8d81-862a80464252\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.712668</td>\n",
              "      <td>0.923184</td>\n",
              "      <td>1.024419</td>\n",
              "      <td>0.801439</td>\n",
              "      <td>0.618942</td>\n",
              "      <td>0.636236</td>\n",
              "      <td>0.614226</td>\n",
              "      <td>0.682791</td>\n",
              "      <td>0.590947</td>\n",
              "      <td>0.725412</td>\n",
              "      <td>...</td>\n",
              "      <td>0.331871</td>\n",
              "      <td>0.310197</td>\n",
              "      <td>0.315931</td>\n",
              "      <td>0.256804</td>\n",
              "      <td>0.453488</td>\n",
              "      <td>0.373803</td>\n",
              "      <td>0.373036</td>\n",
              "      <td>0.321717</td>\n",
              "      <td>0.380656</td>\n",
              "      <td>0.310440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.747238</td>\n",
              "      <td>1.138564</td>\n",
              "      <td>2.041946</td>\n",
              "      <td>1.846981</td>\n",
              "      <td>1.354905</td>\n",
              "      <td>1.211606</td>\n",
              "      <td>1.750458</td>\n",
              "      <td>1.850202</td>\n",
              "      <td>1.517437</td>\n",
              "      <td>1.605451</td>\n",
              "      <td>...</td>\n",
              "      <td>6.425349</td>\n",
              "      <td>6.824083</td>\n",
              "      <td>4.653356</td>\n",
              "      <td>5.980423</td>\n",
              "      <td>5.137825</td>\n",
              "      <td>5.934677</td>\n",
              "      <td>4.503961</td>\n",
              "      <td>5.772872</td>\n",
              "      <td>5.964221</td>\n",
              "      <td>5.707910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.595745</td>\n",
              "      <td>0.804496</td>\n",
              "      <td>0.809715</td>\n",
              "      <td>0.855078</td>\n",
              "      <td>0.874749</td>\n",
              "      <td>0.875953</td>\n",
              "      <td>0.892011</td>\n",
              "      <td>0.894018</td>\n",
              "      <td>0.901646</td>\n",
              "      <td>0.895223</td>\n",
              "      <td>...</td>\n",
              "      <td>0.979526</td>\n",
              "      <td>0.977519</td>\n",
              "      <td>0.978723</td>\n",
              "      <td>0.983139</td>\n",
              "      <td>0.976716</td>\n",
              "      <td>0.981132</td>\n",
              "      <td>0.974709</td>\n",
              "      <td>0.979928</td>\n",
              "      <td>0.977921</td>\n",
              "      <td>0.983541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.727067</td>\n",
              "      <td>0.784824</td>\n",
              "      <td>0.725934</td>\n",
              "      <td>0.748584</td>\n",
              "      <td>0.782559</td>\n",
              "      <td>0.828992</td>\n",
              "      <td>0.808607</td>\n",
              "      <td>0.789355</td>\n",
              "      <td>0.840317</td>\n",
              "      <td>0.825595</td>\n",
              "      <td>...</td>\n",
              "      <td>0.853907</td>\n",
              "      <td>0.844847</td>\n",
              "      <td>0.874292</td>\n",
              "      <td>0.866365</td>\n",
              "      <td>0.877690</td>\n",
              "      <td>0.867497</td>\n",
              "      <td>0.886750</td>\n",
              "      <td>0.856172</td>\n",
              "      <td>0.866365</td>\n",
              "      <td>0.859570</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows  160 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b38c18be-ce71-42ca-8d81-862a80464252')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b38c18be-ce71-42ca-8d81-862a80464252 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b38c18be-ce71-42ca-8d81-862a80464252');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  3.712668  0.923184  1.024419  0.801439  0.618942  0.636236  0.614226   \n",
              "1  1.747238  1.138564  2.041946  1.846981  1.354905  1.211606  1.750458   \n",
              "2  0.595745  0.804496  0.809715  0.855078  0.874749  0.875953  0.892011   \n",
              "3  0.727067  0.784824  0.725934  0.748584  0.782559  0.828992  0.808607   \n",
              "\n",
              "        7         8         9    ...       150       151       152       153  \\\n",
              "0  0.682791  0.590947  0.725412  ...  0.331871  0.310197  0.315931  0.256804   \n",
              "1  1.850202  1.517437  1.605451  ...  6.425349  6.824083  4.653356  5.980423   \n",
              "2  0.894018  0.901646  0.895223  ...  0.979526  0.977519  0.978723  0.983139   \n",
              "3  0.789355  0.840317  0.825595  ...  0.853907  0.844847  0.874292  0.866365   \n",
              "\n",
              "        154       155       156       157       158       159  \n",
              "0  0.453488  0.373803  0.373036  0.321717  0.380656  0.310440  \n",
              "1  5.137825  5.934677  4.503961  5.772872  5.964221  5.707910  \n",
              "2  0.976716  0.981132  0.974709  0.979928  0.977921  0.983541  \n",
              "3  0.877690  0.867497  0.886750  0.856172  0.866365  0.859570  \n",
              "\n",
              "[4 rows x 160 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "df=pd.DataFrame(lst)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bktf9F5xDQzD"
      },
      "outputs": [],
      "source": [
        "df.to_csv('Inception.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
