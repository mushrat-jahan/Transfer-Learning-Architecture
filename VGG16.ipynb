{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWlwYlV4ImBs",
        "outputId": "5b004ab8-8970-488d-eccb-82d0bba7b6d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jul  8 10:27:47 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibkJ0a9Az85H",
        "outputId": "f64aa146-5fde-4158-d1e5-a864c274605e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhJif0LeJBDd",
        "outputId": "d301d9bc-42e8-4968-a503-c2ca4b2f9034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 100356    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,815,044\n",
            "Trainable params: 100,356\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Loading deep learning algorithm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "#import keras\n",
        "#import keras.backend as K\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "IMAGE_SIZE = [224,224]\n",
        "CLASS=4\n",
        "inception = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "#model = VGG16(weights='imagenet', include_top=False)\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False\n",
        "#folders = glob('C:\\rafid\\guava disease research\\k_guava\\train_image/*')\n",
        "x = Flatten()(inception.output)\n",
        "prediction = Dense(CLASS, activation='softmax')(x)\n",
        "model = Model(inputs=inception.input, outputs=prediction)\n",
        "adam = keras.optimizers.Adam(lr = 0.001)\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer = adam,\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "print(\"\\n\\n\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zi2BVgcJBH9",
        "outputId": "fcc36c81-d262-4b34-8feb-36dedaaac806"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/defense/Split-dataset/train',\n",
        "                                                 target_size = (224,224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "val_set = val_datagen.flow_from_directory('/content/drive/MyDrive/defense/Split-dataset/val',\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/defense/Split-dataset/test',\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 1,\n",
        "                                            class_mode = 'categorical')\n",
        "print(\"\\n\\n\")\n",
        "model.optimizer.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PFBfidKCC_p9",
        "outputId": "4e327f15-0b46-4005-fa22-8b7707bccbd1"
      },
      "outputs": [],
      "source": [
        "filepath = \"/content/drive/MyDrive/defense/Accuracy/vgg16/highest_val2.h5\"\n",
        "filepath2 = \"/content/drive/MyDrive/defense/Accuracy/vgg16/highest_train2.h5\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,\n",
        "                             save_best_only=True, mode='max')\n",
        "checkpoint2 = ModelCheckpoint(filepath2, monitor='accuracy', verbose=1,\n",
        "                             save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1,checkpoint2]\n",
        "r = model.fit_generator(\n",
        "    training_set,\n",
        "    epochs=160,\n",
        "    validation_data=val_set,\n",
        "    steps_per_epoch=len(training_set),\n",
        "    validation_steps=len(val_set),\n",
        "    callbacks=callbacks_list\n",
        ")\n",
        "model.save_weights(\"/content/drive/MyDrive/defense/Accuracy/vgg166/end2.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "gTQSsVz7JBNm",
        "outputId": "26361e8f-ebd6-4402-9d21-621877efa1ac"
      },
      "outputs": [],
      "source": [
        "#plot of accuracy and loss\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "# plot the loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.title('Training and validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "# plot the accuracy\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FEg7fTCJBQo",
        "outputId": "bbf53444-6da3-471d-da00-0f3e2a2470c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2006: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss = 0.6372169852256775\n",
            "Test Accuracy = 0.8952381014823914\n"
          ]
        }
      ],
      "source": [
        "#evaluating the model (test acc)\n",
        "#batch size = 32\n",
        "model.load_weights('/content/drive/MyDrive/Onion Diseases/Accuracy /VGG16/highest_val2.h5')\n",
        "preds = model.evaluate_generator(test_set)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "bUF16_nnJOiK",
        "outputId": "c6876485-c3ca-402f-889d-0029c0e9b4c2"
      },
      "outputs": [],
      "source": [
        "#confusion matrix\n",
        "\n",
        "#you have to set test bath size=1 before running the cell\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import tensorflow as tf\n",
        "model.load_weights('/content/drive/MyDrive/Onion Diseases/Accuracy /VGG16/highest_val2.h5')\n",
        "filenames=test_set.filenames\n",
        "nb_samples=len(test_set)\n",
        "y_prob=[]\n",
        "y_act=[]\n",
        "test_set.reset()\n",
        "for _ in range (nb_samples):\n",
        "    X_test,Y_test = test_set.next()\n",
        "    y_prob.append(model.predict(X_test))\n",
        "    y_act.append(Y_test)\n",
        "predicted_class=[list(training_set.class_indices.keys())[i.argmax()] for i in y_prob]\n",
        "actual_class=[list(training_set.class_indices.keys())[i.argmax()]for i in y_act]\n",
        "out_df=pd.DataFrame(np.vstack([predicted_class,actual_class]).T,columns=['predicted_class','actual_class'])\n",
        "confusion_matrix=pd.crosstab(out_df['actual_class'],out_df['predicted_class'],rownames=['Actual'],colnames=['Predicted'])\n",
        "import matplotlib.pyplot as plt\n",
        "sn.heatmap(confusion_matrix,cmap='flare', annot=True, fmt='d')\n",
        "plt.show()\n",
        "#plt.savefig('/content/drive/MyDrive/model weights/vgg16_AugGfb_split1_maxval_3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9b008ssJOlH"
      },
      "outputs": [],
      "source": [
        "lst=[r.history['loss'],r.history['val_loss'],r.history['accuracy'],r.history['val_accuracy']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "uuBC62vWJOtM",
        "outputId": "b07fe732-7c6b-48bc-e98b-a55991449f06"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.063946</td>\n",
              "      <td>0.562295</td>\n",
              "      <td>0.419263</td>\n",
              "      <td>0.332225</td>\n",
              "      <td>0.281066</td>\n",
              "      <td>0.250044</td>\n",
              "      <td>0.227190</td>\n",
              "      <td>0.205565</td>\n",
              "      <td>0.164640</td>\n",
              "      <td>0.161166</td>\n",
              "      <td>0.150859</td>\n",
              "      <td>0.135544</td>\n",
              "      <td>0.137505</td>\n",
              "      <td>0.127168</td>\n",
              "      <td>0.121686</td>\n",
              "      <td>0.103266</td>\n",
              "      <td>0.081665</td>\n",
              "      <td>0.113968</td>\n",
              "      <td>0.083766</td>\n",
              "      <td>0.086503</td>\n",
              "      <td>0.071566</td>\n",
              "      <td>0.068733</td>\n",
              "      <td>0.065447</td>\n",
              "      <td>0.072682</td>\n",
              "      <td>0.053664</td>\n",
              "      <td>0.074712</td>\n",
              "      <td>0.053769</td>\n",
              "      <td>0.058828</td>\n",
              "      <td>0.066965</td>\n",
              "      <td>0.055327</td>\n",
              "      <td>0.066766</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.057807</td>\n",
              "      <td>0.063958</td>\n",
              "      <td>0.060098</td>\n",
              "      <td>0.058977</td>\n",
              "      <td>0.057235</td>\n",
              "      <td>0.049694</td>\n",
              "      <td>0.056211</td>\n",
              "      <td>0.037851</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.025073</td>\n",
              "      <td>0.037075</td>\n",
              "      <td>0.024423</td>\n",
              "      <td>0.032857</td>\n",
              "      <td>0.025307</td>\n",
              "      <td>0.037053</td>\n",
              "      <td>0.021943</td>\n",
              "      <td>0.048483</td>\n",
              "      <td>0.052883</td>\n",
              "      <td>0.036644</td>\n",
              "      <td>0.025488</td>\n",
              "      <td>0.026418</td>\n",
              "      <td>0.017771</td>\n",
              "      <td>0.024358</td>\n",
              "      <td>0.028515</td>\n",
              "      <td>0.040957</td>\n",
              "      <td>0.037452</td>\n",
              "      <td>0.033422</td>\n",
              "      <td>0.019166</td>\n",
              "      <td>0.022417</td>\n",
              "      <td>0.029063</td>\n",
              "      <td>0.013146</td>\n",
              "      <td>0.028047</td>\n",
              "      <td>0.024241</td>\n",
              "      <td>0.017351</td>\n",
              "      <td>0.031081</td>\n",
              "      <td>0.040792</td>\n",
              "      <td>0.025646</td>\n",
              "      <td>0.046916</td>\n",
              "      <td>0.042484</td>\n",
              "      <td>0.024934</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>0.017365</td>\n",
              "      <td>0.019025</td>\n",
              "      <td>0.036077</td>\n",
              "      <td>0.036043</td>\n",
              "      <td>0.022657</td>\n",
              "      <td>0.025956</td>\n",
              "      <td>0.054078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.744979</td>\n",
              "      <td>0.635575</td>\n",
              "      <td>0.424732</td>\n",
              "      <td>0.472201</td>\n",
              "      <td>0.387601</td>\n",
              "      <td>0.390967</td>\n",
              "      <td>0.411083</td>\n",
              "      <td>0.355824</td>\n",
              "      <td>0.317913</td>\n",
              "      <td>0.318755</td>\n",
              "      <td>0.324523</td>\n",
              "      <td>0.389066</td>\n",
              "      <td>0.313781</td>\n",
              "      <td>0.337654</td>\n",
              "      <td>0.329102</td>\n",
              "      <td>0.318284</td>\n",
              "      <td>0.392043</td>\n",
              "      <td>0.310019</td>\n",
              "      <td>0.308675</td>\n",
              "      <td>0.318253</td>\n",
              "      <td>0.310531</td>\n",
              "      <td>0.399959</td>\n",
              "      <td>0.301473</td>\n",
              "      <td>0.370622</td>\n",
              "      <td>0.336134</td>\n",
              "      <td>0.301352</td>\n",
              "      <td>0.342349</td>\n",
              "      <td>0.309791</td>\n",
              "      <td>0.324641</td>\n",
              "      <td>0.318276</td>\n",
              "      <td>0.378463</td>\n",
              "      <td>0.363966</td>\n",
              "      <td>0.370685</td>\n",
              "      <td>0.359754</td>\n",
              "      <td>0.444659</td>\n",
              "      <td>0.367711</td>\n",
              "      <td>0.449735</td>\n",
              "      <td>0.381284</td>\n",
              "      <td>0.357317</td>\n",
              "      <td>0.379763</td>\n",
              "      <td>...</td>\n",
              "      <td>0.483296</td>\n",
              "      <td>0.479123</td>\n",
              "      <td>0.674842</td>\n",
              "      <td>0.523996</td>\n",
              "      <td>0.478137</td>\n",
              "      <td>0.472133</td>\n",
              "      <td>0.528279</td>\n",
              "      <td>0.665578</td>\n",
              "      <td>0.725088</td>\n",
              "      <td>0.707837</td>\n",
              "      <td>0.535278</td>\n",
              "      <td>0.653428</td>\n",
              "      <td>0.550126</td>\n",
              "      <td>0.530339</td>\n",
              "      <td>0.666191</td>\n",
              "      <td>0.774262</td>\n",
              "      <td>0.783743</td>\n",
              "      <td>0.787930</td>\n",
              "      <td>0.497952</td>\n",
              "      <td>0.552518</td>\n",
              "      <td>0.590774</td>\n",
              "      <td>0.530985</td>\n",
              "      <td>0.605204</td>\n",
              "      <td>0.754556</td>\n",
              "      <td>0.571858</td>\n",
              "      <td>0.571094</td>\n",
              "      <td>0.573701</td>\n",
              "      <td>0.627058</td>\n",
              "      <td>0.589159</td>\n",
              "      <td>0.625886</td>\n",
              "      <td>0.552360</td>\n",
              "      <td>0.704162</td>\n",
              "      <td>0.759786</td>\n",
              "      <td>0.665776</td>\n",
              "      <td>0.680469</td>\n",
              "      <td>0.590573</td>\n",
              "      <td>0.577251</td>\n",
              "      <td>0.520232</td>\n",
              "      <td>0.625593</td>\n",
              "      <td>0.676441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.575404</td>\n",
              "      <td>0.789763</td>\n",
              "      <td>0.859842</td>\n",
              "      <td>0.892477</td>\n",
              "      <td>0.912058</td>\n",
              "      <td>0.921333</td>\n",
              "      <td>0.926829</td>\n",
              "      <td>0.935761</td>\n",
              "      <td>0.953624</td>\n",
              "      <td>0.953281</td>\n",
              "      <td>0.954998</td>\n",
              "      <td>0.962899</td>\n",
              "      <td>0.954655</td>\n",
              "      <td>0.960838</td>\n",
              "      <td>0.964273</td>\n",
              "      <td>0.970457</td>\n",
              "      <td>0.982824</td>\n",
              "      <td>0.965991</td>\n",
              "      <td>0.976984</td>\n",
              "      <td>0.976297</td>\n",
              "      <td>0.981106</td>\n",
              "      <td>0.984541</td>\n",
              "      <td>0.982824</td>\n",
              "      <td>0.981106</td>\n",
              "      <td>0.987290</td>\n",
              "      <td>0.980419</td>\n",
              "      <td>0.987977</td>\n",
              "      <td>0.984885</td>\n",
              "      <td>0.982137</td>\n",
              "      <td>0.984885</td>\n",
              "      <td>0.984541</td>\n",
              "      <td>0.980076</td>\n",
              "      <td>0.983854</td>\n",
              "      <td>0.981450</td>\n",
              "      <td>0.979732</td>\n",
              "      <td>0.980076</td>\n",
              "      <td>0.986603</td>\n",
              "      <td>0.986946</td>\n",
              "      <td>0.985572</td>\n",
              "      <td>0.989694</td>\n",
              "      <td>...</td>\n",
              "      <td>0.989694</td>\n",
              "      <td>0.992442</td>\n",
              "      <td>0.989007</td>\n",
              "      <td>0.992786</td>\n",
              "      <td>0.990381</td>\n",
              "      <td>0.992442</td>\n",
              "      <td>0.990381</td>\n",
              "      <td>0.992442</td>\n",
              "      <td>0.986259</td>\n",
              "      <td>0.983511</td>\n",
              "      <td>0.989007</td>\n",
              "      <td>0.992442</td>\n",
              "      <td>0.993817</td>\n",
              "      <td>0.994160</td>\n",
              "      <td>0.995534</td>\n",
              "      <td>0.990381</td>\n",
              "      <td>0.989694</td>\n",
              "      <td>0.987977</td>\n",
              "      <td>0.991068</td>\n",
              "      <td>0.995534</td>\n",
              "      <td>0.994160</td>\n",
              "      <td>0.993817</td>\n",
              "      <td>0.995191</td>\n",
              "      <td>0.992442</td>\n",
              "      <td>0.992786</td>\n",
              "      <td>0.994847</td>\n",
              "      <td>0.989694</td>\n",
              "      <td>0.989694</td>\n",
              "      <td>0.995191</td>\n",
              "      <td>0.986603</td>\n",
              "      <td>0.987977</td>\n",
              "      <td>0.993129</td>\n",
              "      <td>0.992442</td>\n",
              "      <td>0.994847</td>\n",
              "      <td>0.994504</td>\n",
              "      <td>0.990381</td>\n",
              "      <td>0.988320</td>\n",
              "      <td>0.993473</td>\n",
              "      <td>0.993129</td>\n",
              "      <td>0.983511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.711191</td>\n",
              "      <td>0.778580</td>\n",
              "      <td>0.849579</td>\n",
              "      <td>0.820698</td>\n",
              "      <td>0.858002</td>\n",
              "      <td>0.851986</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.876053</td>\n",
              "      <td>0.877256</td>\n",
              "      <td>0.888087</td>\n",
              "      <td>0.883273</td>\n",
              "      <td>0.876053</td>\n",
              "      <td>0.885680</td>\n",
              "      <td>0.889290</td>\n",
              "      <td>0.896510</td>\n",
              "      <td>0.904934</td>\n",
              "      <td>0.866426</td>\n",
              "      <td>0.896510</td>\n",
              "      <td>0.889290</td>\n",
              "      <td>0.882070</td>\n",
              "      <td>0.895307</td>\n",
              "      <td>0.889290</td>\n",
              "      <td>0.896510</td>\n",
              "      <td>0.873646</td>\n",
              "      <td>0.897714</td>\n",
              "      <td>0.897714</td>\n",
              "      <td>0.890493</td>\n",
              "      <td>0.909747</td>\n",
              "      <td>0.894103</td>\n",
              "      <td>0.897714</td>\n",
              "      <td>0.878460</td>\n",
              "      <td>0.888087</td>\n",
              "      <td>0.878460</td>\n",
              "      <td>0.883273</td>\n",
              "      <td>0.879663</td>\n",
              "      <td>0.892900</td>\n",
              "      <td>0.873646</td>\n",
              "      <td>0.886883</td>\n",
              "      <td>0.890493</td>\n",
              "      <td>0.892900</td>\n",
              "      <td>...</td>\n",
              "      <td>0.902527</td>\n",
              "      <td>0.900120</td>\n",
              "      <td>0.866426</td>\n",
              "      <td>0.897714</td>\n",
              "      <td>0.900120</td>\n",
              "      <td>0.906137</td>\n",
              "      <td>0.894103</td>\n",
              "      <td>0.882070</td>\n",
              "      <td>0.884477</td>\n",
              "      <td>0.886883</td>\n",
              "      <td>0.892900</td>\n",
              "      <td>0.885680</td>\n",
              "      <td>0.889290</td>\n",
              "      <td>0.906137</td>\n",
              "      <td>0.883273</td>\n",
              "      <td>0.867629</td>\n",
              "      <td>0.865223</td>\n",
              "      <td>0.876053</td>\n",
              "      <td>0.910951</td>\n",
              "      <td>0.892900</td>\n",
              "      <td>0.889290</td>\n",
              "      <td>0.903730</td>\n",
              "      <td>0.886883</td>\n",
              "      <td>0.879663</td>\n",
              "      <td>0.888087</td>\n",
              "      <td>0.888087</td>\n",
              "      <td>0.900120</td>\n",
              "      <td>0.888087</td>\n",
              "      <td>0.885680</td>\n",
              "      <td>0.889290</td>\n",
              "      <td>0.892900</td>\n",
              "      <td>0.886883</td>\n",
              "      <td>0.876053</td>\n",
              "      <td>0.884477</td>\n",
              "      <td>0.886883</td>\n",
              "      <td>0.894103</td>\n",
              "      <td>0.890493</td>\n",
              "      <td>0.902527</td>\n",
              "      <td>0.884477</td>\n",
              "      <td>0.891697</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 160 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       157       158       159\n",
              "0  1.063946  0.562295  0.419263  ...  0.022657  0.025956  0.054078\n",
              "1  0.744979  0.635575  0.424732  ...  0.520232  0.625593  0.676441\n",
              "2  0.575404  0.789763  0.859842  ...  0.993473  0.993129  0.983511\n",
              "3  0.711191  0.778580  0.849579  ...  0.902527  0.884477  0.891697\n",
              "\n",
              "[4 rows x 160 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(lst)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqA6E_E-2LA7"
      },
      "outputs": [],
      "source": [
        "df.to_csv('IRNadam.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
